{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImLCXm8IsSS2"
      },
      "source": [
        "# Download the Cora data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRN47p1SKRgP"
      },
      "outputs": [],
      "source": [
        "! wget https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
        "! tar -zxvf cora.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXIYzURA4OKg"
      },
      "source": [
        "# import modules and set random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJQYMX02_z0M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import pandas as pd\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "seed = 0\n",
        "\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgOv1h7YsK-5"
      },
      "source": [
        "# Loading and preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXPHN61i9keB"
      },
      "outputs": [],
      "source": [
        "def encode_onehot(labels):\n",
        "    # The classes must be sorted before encoding to enable static class encoding.\n",
        "    # In other words, make sure the first class always maps to index 0.\n",
        "    classes = sorted(list(set(labels)))\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
        "                    enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
        "                             dtype=np.int32)\n",
        "    return labels_onehot\n",
        "\n",
        "\n",
        "def load_data(path=\"/content/cora/\", dataset=\"cora\", training_samples=140):\n",
        "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
        "    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
        "                                        dtype=np.dtype(str))\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "    labels = encode_onehot(idx_features_labels[:, -1])\n",
        "\n",
        "    # build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
        "                                    dtype=np.int32)\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
        "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "                        shape=(labels.shape[0], labels.shape[0]),\n",
        "                        dtype=np.float32)\n",
        "\n",
        "    # build symmetric adjacency matrix\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    features = normalize(features)\n",
        "    adj = adj + sp.eye(adj.shape[0])\n",
        "    adj = normalize_adj(adj)\n",
        "\n",
        "    # Random indexes\n",
        "    idx_rand = torch.randperm(len(labels))\n",
        "    # Nodes for training\n",
        "    idx_train = idx_rand[:training_samples]\n",
        "    # Nodes for validation\n",
        "    idx_val= idx_rand[training_samples:]\n",
        "\n",
        "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val\n",
        "\n",
        "def normalize_adj(mx):\n",
        "    \"\"\"symmetric normalization\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
        "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
        "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
        "\n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzCZVd1JsbHr"
      },
      "source": [
        "## check the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlsKjMKx8_b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53b71907-e984-4fb3-86fb-f3040d404cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n"
          ]
        }
      ],
      "source": [
        "adj, features, labels, idx_train, idx_val = load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxrv21rLnpiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77c2adaa-54d5-47a5-b8d2-4e9ad010b2ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1667, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.2000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.2000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2500]])\n",
            "torch.Size([2708, 2708])\n"
          ]
        }
      ],
      "source": [
        "print(adj)\n",
        "print(adj.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWrDf0iWnpqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "465eeecb-eec1-4dd6-c25e-d2f0aaf7a1c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "torch.Size([2708, 1433])\n"
          ]
        }
      ],
      "source": [
        "print(features)\n",
        "print(features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUkt2JJdsuA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b81b9a9-78e4-4c13-822b-6bcc52dcf9d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 5, 4,  ..., 1, 0, 2])\n",
            "tensor([0, 1, 2, 3, 4, 5, 6])\n",
            "2708\n"
          ]
        }
      ],
      "source": [
        "print(labels)\n",
        "print(labels.unique())\n",
        "print(len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGP18jNAs1Gp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f0a07fb-ed34-45d1-f78c-3350c3819498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140\n",
            "2568\n"
          ]
        }
      ],
      "source": [
        "print(len(idx_train))\n",
        "print(len(idx_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHqIcfH-vIic"
      },
      "source": [
        "# Vanilla GCN for node classification\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Graph Convolution layer (Your Task)\n",
        "\n",
        "This module takes $\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$ where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input and outputs $\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$, where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$.\n",
        "1.   perform initial transformation: $\\mathbf{s} = \\mathbf{W} \\times \\mathbf{h} ^{(l)}$\n",
        "2.   multiply $\\mathbf{s}$ by normalized adjacency matrix: $\\mathbf{h'} = \\mathbf{A} \\times \\mathbf{s}$"
      ],
      "metadata": {
        "id": "f48tylWyjLPE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-fU8L7f41VZ"
      },
      "outputs": [],
      "source": [
        "class GraphConvolution(nn.Module):\n",
        "    \"\"\"\n",
        "    A Graph Convolution Layer (GCN)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        \"\"\"\n",
        "        * `in_features`, $F$, is the number of input features per node\n",
        "        * `out_features`, $F'$, is the number of output features per node\n",
        "        * `bias`, whether to include the bias term in the linear layer. Default=True\n",
        "        \"\"\"\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        # TODO: initialize the weight W that maps the input feature (dim F ) to output feature (dim F')\n",
        "        # hint: use nn.Linear()\n",
        "        ############ Your code here ###################################\n",
        "        self.W = nn.Linear(in_features=in_features, out_features=out_features, bias=bias)\n",
        "        ###############################################################\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        # TODO: transform input feature to output (don't forget to use the adjacency matrix\n",
        "        # to sum over neighbouring nodes )\n",
        "        # hint: use the linear layer you declared above.\n",
        "        # hint: you can use torch.spmm() sparse matrix multiplication to handle the\n",
        "        #       adjacency matrix\n",
        "        ############ Your code here ###################################\n",
        "        s = self.W(input)\n",
        "        return torch.spmm(adj, s)\n",
        "        ###############################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define GCN (Your Task)\n",
        "\n",
        "you will implement a two-layer GCN with ReLU activation function and Dropout after the first Conv layer."
      ],
      "metadata": {
        "id": "RxBELCxkjF6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(nn.Module):\n",
        "    '''\n",
        "    A two-layer GCN\n",
        "    '''\n",
        "    def __init__(self, nfeat, n_hidden, n_classes, dropout, bias=True):\n",
        "        \"\"\"\n",
        "        * `nfeat`, is the number of input features per node of the first layer\n",
        "        * `n_hidden`, number of hidden units\n",
        "        * `n_classes`, total number of classes for classification\n",
        "        * `dropout`, the dropout ratio\n",
        "        * `bias`, whether to include the bias term in the linear layer. Default=True\n",
        "        \"\"\"\n",
        "\n",
        "        super(GCN, self).__init__()\n",
        "        # TODO: Initialization\n",
        "        # (1) 2 GraphConvolution() layers.\n",
        "        # (2) 1 Dropout layer\n",
        "        # (3) 1 activation function: ReLU()\n",
        "        ############ Your code here ###################################\n",
        "        self.graphconv1 = GraphConvolution(nfeat, n_hidden, bias)\n",
        "        self.graphconv2 = GraphConvolution(n_hidden, n_classes, bias)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.relu = nn.ReLU()\n",
        "        ###############################################################\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        # TODO: the input will pass through the first graph convolution layer,\n",
        "        # the activation function, the dropout layer, then the second graph\n",
        "        # convolution layer. No activation function for the\n",
        "        # last layer. Return the logits.\n",
        "        ############ Your code here ###################################\n",
        "\n",
        "        results = self.graphconv2(self.dropout(self.relu(self.graphconv1(x, adj))), adj)\n",
        "        return results\n",
        "\n",
        "        ###############################################################"
      ],
      "metadata": {
        "id": "HtVr2cN8jD5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX1d9F1G508r"
      },
      "source": [
        "## define loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyhqJ39OCzNN"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXsdid6C5K1c"
      },
      "source": [
        "## training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjlYeoFPFAWm"
      },
      "outputs": [],
      "source": [
        "args = {\"training_samples\": 140,\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 0.01,\n",
        "        \"weight_decay\": 5e-4,\n",
        "        \"hidden\": 16,\n",
        "        \"dropout\": 0.5,\n",
        "        \"bias\": True,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qbx0uc-9G5vs"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(features, adj)\n",
        "    loss_train = criterion(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "\n",
        "    loss_val = criterion(output[idx_val], labels[idx_val])\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_test = criterion(output[idx_val], labels[idx_val])\n",
        "    acc_test = accuracy(output[idx_val], labels[idx_val])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjNiui83FYBr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25847ac9-26cd-4844-cb29-a35b5e75e30d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n"
          ]
        }
      ],
      "source": [
        "model = GCN(nfeat=features.shape[1],\n",
        "            n_hidden=args[\"hidden\"],\n",
        "            n_classes=labels.max().item() + 1,\n",
        "            dropout=args[\"dropout\"]).to(device)\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n",
        "\n",
        "\n",
        "adj, features, labels, idx_train, idx_val = load_data(training_samples=args[\"training_samples\"])\n",
        "adj, features, labels, idx_train, idx_val = adj.to(device), features.to(device), labels.to(device), idx_train.to(device), idx_val.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training Vanilla GCN"
      ],
      "metadata": {
        "id": "1W6tqqj16iz-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSjUYJPSlnOU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef67f6c-e6fb-4462-8dca-a96a7544b370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss_train: 1.9660 acc_train: 0.1357 loss_val: 1.9520 acc_val: 0.1554 time: 0.0109s\n",
            "Epoch: 0002 loss_train: 1.9574 acc_train: 0.1357 loss_val: 1.9445 acc_val: 0.1554 time: 0.0096s\n",
            "Epoch: 0003 loss_train: 1.9483 acc_train: 0.1357 loss_val: 1.9358 acc_val: 0.1554 time: 0.0101s\n",
            "Epoch: 0004 loss_train: 1.9356 acc_train: 0.1357 loss_val: 1.9264 acc_val: 0.1554 time: 0.0107s\n",
            "Epoch: 0005 loss_train: 1.9255 acc_train: 0.1429 loss_val: 1.9164 acc_val: 0.1713 time: 0.0112s\n",
            "Epoch: 0006 loss_train: 1.9096 acc_train: 0.2143 loss_val: 1.9059 acc_val: 0.3941 time: 0.0098s\n",
            "Epoch: 0007 loss_train: 1.8959 acc_train: 0.3429 loss_val: 1.8949 acc_val: 0.3057 time: 0.0104s\n",
            "Epoch: 0008 loss_train: 1.8832 acc_train: 0.3357 loss_val: 1.8838 acc_val: 0.3026 time: 0.0113s\n",
            "Epoch: 0009 loss_train: 1.8680 acc_train: 0.2929 loss_val: 1.8732 acc_val: 0.3026 time: 0.0110s\n",
            "Epoch: 0010 loss_train: 1.8550 acc_train: 0.2929 loss_val: 1.8628 acc_val: 0.3026 time: 0.0095s\n",
            "Epoch: 0011 loss_train: 1.8339 acc_train: 0.3000 loss_val: 1.8527 acc_val: 0.3026 time: 0.0117s\n",
            "Epoch: 0012 loss_train: 1.8203 acc_train: 0.3143 loss_val: 1.8432 acc_val: 0.3026 time: 0.0092s\n",
            "Epoch: 0013 loss_train: 1.8098 acc_train: 0.3071 loss_val: 1.8343 acc_val: 0.3030 time: 0.0100s\n",
            "Epoch: 0014 loss_train: 1.8000 acc_train: 0.3429 loss_val: 1.8261 acc_val: 0.3033 time: 0.0108s\n",
            "Epoch: 0015 loss_train: 1.7788 acc_train: 0.3143 loss_val: 1.8186 acc_val: 0.3033 time: 0.0100s\n",
            "Epoch: 0016 loss_train: 1.7616 acc_train: 0.3214 loss_val: 1.8118 acc_val: 0.3030 time: 0.0097s\n",
            "Epoch: 0017 loss_train: 1.7641 acc_train: 0.3000 loss_val: 1.8055 acc_val: 0.3030 time: 0.0102s\n",
            "Epoch: 0018 loss_train: 1.7451 acc_train: 0.3071 loss_val: 1.7995 acc_val: 0.3026 time: 0.0100s\n",
            "Epoch: 0019 loss_train: 1.7285 acc_train: 0.3143 loss_val: 1.7934 acc_val: 0.3026 time: 0.0116s\n",
            "Epoch: 0020 loss_train: 1.7233 acc_train: 0.3143 loss_val: 1.7872 acc_val: 0.3026 time: 0.0100s\n",
            "Epoch: 0021 loss_train: 1.7216 acc_train: 0.3000 loss_val: 1.7808 acc_val: 0.3026 time: 0.0103s\n",
            "Epoch: 0022 loss_train: 1.7062 acc_train: 0.3143 loss_val: 1.7743 acc_val: 0.3045 time: 0.0097s\n",
            "Epoch: 0023 loss_train: 1.6912 acc_train: 0.3357 loss_val: 1.7677 acc_val: 0.3045 time: 0.0114s\n",
            "Epoch: 0024 loss_train: 1.6897 acc_train: 0.3286 loss_val: 1.7610 acc_val: 0.3049 time: 0.0086s\n",
            "Epoch: 0025 loss_train: 1.6627 acc_train: 0.3286 loss_val: 1.7541 acc_val: 0.3053 time: 0.0107s\n",
            "Epoch: 0026 loss_train: 1.6456 acc_train: 0.3500 loss_val: 1.7472 acc_val: 0.3072 time: 0.0116s\n",
            "Epoch: 0027 loss_train: 1.6618 acc_train: 0.3286 loss_val: 1.7404 acc_val: 0.3123 time: 0.0087s\n",
            "Epoch: 0028 loss_train: 1.6403 acc_train: 0.3357 loss_val: 1.7335 acc_val: 0.3193 time: 0.0129s\n",
            "Epoch: 0029 loss_train: 1.6224 acc_train: 0.3643 loss_val: 1.7267 acc_val: 0.3267 time: 0.0116s\n",
            "Epoch: 0030 loss_train: 1.6061 acc_train: 0.3786 loss_val: 1.7197 acc_val: 0.3322 time: 0.0105s\n",
            "Epoch: 0031 loss_train: 1.5956 acc_train: 0.3571 loss_val: 1.7127 acc_val: 0.3392 time: 0.0109s\n",
            "Epoch: 0032 loss_train: 1.5961 acc_train: 0.4143 loss_val: 1.7055 acc_val: 0.3567 time: 0.0108s\n",
            "Epoch: 0033 loss_train: 1.5809 acc_train: 0.4071 loss_val: 1.6978 acc_val: 0.3707 time: 0.0106s\n",
            "Epoch: 0034 loss_train: 1.5724 acc_train: 0.4143 loss_val: 1.6898 acc_val: 0.3754 time: 0.0105s\n",
            "Epoch: 0035 loss_train: 1.5561 acc_train: 0.4143 loss_val: 1.6814 acc_val: 0.3801 time: 0.0098s\n",
            "Epoch: 0036 loss_train: 1.5289 acc_train: 0.4500 loss_val: 1.6727 acc_val: 0.3840 time: 0.0099s\n",
            "Epoch: 0037 loss_train: 1.5138 acc_train: 0.4571 loss_val: 1.6637 acc_val: 0.3882 time: 0.0111s\n",
            "Epoch: 0038 loss_train: 1.4821 acc_train: 0.4643 loss_val: 1.6543 acc_val: 0.3906 time: 0.0101s\n",
            "Epoch: 0039 loss_train: 1.4736 acc_train: 0.4857 loss_val: 1.6446 acc_val: 0.3995 time: 0.0115s\n",
            "Epoch: 0040 loss_train: 1.4572 acc_train: 0.5286 loss_val: 1.6347 acc_val: 0.4089 time: 0.0097s\n",
            "Epoch: 0041 loss_train: 1.4599 acc_train: 0.5071 loss_val: 1.6245 acc_val: 0.4229 time: 0.0098s\n",
            "Epoch: 0042 loss_train: 1.4261 acc_train: 0.5571 loss_val: 1.6141 acc_val: 0.4319 time: 0.0095s\n",
            "Epoch: 0043 loss_train: 1.4191 acc_train: 0.5929 loss_val: 1.6039 acc_val: 0.4400 time: 0.0099s\n",
            "Epoch: 0044 loss_train: 1.3971 acc_train: 0.5857 loss_val: 1.5935 acc_val: 0.4463 time: 0.0108s\n",
            "Epoch: 0045 loss_train: 1.3945 acc_train: 0.6071 loss_val: 1.5833 acc_val: 0.4529 time: 0.0109s\n",
            "Epoch: 0046 loss_train: 1.3877 acc_train: 0.5857 loss_val: 1.5733 acc_val: 0.4587 time: 0.0102s\n",
            "Epoch: 0047 loss_train: 1.3733 acc_train: 0.6357 loss_val: 1.5637 acc_val: 0.4611 time: 0.0094s\n",
            "Epoch: 0048 loss_train: 1.3463 acc_train: 0.6429 loss_val: 1.5540 acc_val: 0.4638 time: 0.0107s\n",
            "Epoch: 0049 loss_train: 1.3060 acc_train: 0.6643 loss_val: 1.5444 acc_val: 0.4661 time: 0.0091s\n",
            "Epoch: 0050 loss_train: 1.3198 acc_train: 0.6357 loss_val: 1.5352 acc_val: 0.4692 time: 0.0106s\n",
            "Epoch: 0051 loss_train: 1.3077 acc_train: 0.6714 loss_val: 1.5260 acc_val: 0.4720 time: 0.0101s\n",
            "Epoch: 0052 loss_train: 1.2870 acc_train: 0.6286 loss_val: 1.5166 acc_val: 0.4755 time: 0.0093s\n",
            "Epoch: 0053 loss_train: 1.2571 acc_train: 0.6500 loss_val: 1.5073 acc_val: 0.4860 time: 0.0097s\n",
            "Epoch: 0054 loss_train: 1.2679 acc_train: 0.6571 loss_val: 1.4976 acc_val: 0.4934 time: 0.0099s\n",
            "Epoch: 0055 loss_train: 1.2684 acc_train: 0.6857 loss_val: 1.4870 acc_val: 0.5039 time: 0.0096s\n",
            "Epoch: 0056 loss_train: 1.2517 acc_train: 0.6786 loss_val: 1.4765 acc_val: 0.5125 time: 0.0123s\n",
            "Epoch: 0057 loss_train: 1.2101 acc_train: 0.6786 loss_val: 1.4661 acc_val: 0.5152 time: 0.0109s\n",
            "Epoch: 0058 loss_train: 1.2039 acc_train: 0.6643 loss_val: 1.4560 acc_val: 0.5210 time: 0.0105s\n",
            "Epoch: 0059 loss_train: 1.1811 acc_train: 0.6929 loss_val: 1.4463 acc_val: 0.5292 time: 0.0094s\n",
            "Epoch: 0060 loss_train: 1.1782 acc_train: 0.6714 loss_val: 1.4368 acc_val: 0.5354 time: 0.0098s\n",
            "Epoch: 0061 loss_train: 1.1631 acc_train: 0.7071 loss_val: 1.4275 acc_val: 0.5405 time: 0.0107s\n",
            "Epoch: 0062 loss_train: 1.1276 acc_train: 0.7071 loss_val: 1.4181 acc_val: 0.5471 time: 0.0086s\n",
            "Epoch: 0063 loss_train: 1.1249 acc_train: 0.7214 loss_val: 1.4092 acc_val: 0.5576 time: 0.0092s\n",
            "Epoch: 0064 loss_train: 1.1172 acc_train: 0.7071 loss_val: 1.4000 acc_val: 0.5615 time: 0.0103s\n",
            "Epoch: 0065 loss_train: 1.1239 acc_train: 0.7071 loss_val: 1.3912 acc_val: 0.5678 time: 0.0096s\n",
            "Epoch: 0066 loss_train: 1.1116 acc_train: 0.7286 loss_val: 1.3819 acc_val: 0.5720 time: 0.0092s\n",
            "Epoch: 0067 loss_train: 1.0874 acc_train: 0.7357 loss_val: 1.3730 acc_val: 0.5771 time: 0.0096s\n",
            "Epoch: 0068 loss_train: 1.0875 acc_train: 0.7286 loss_val: 1.3639 acc_val: 0.5822 time: 0.0104s\n",
            "Epoch: 0069 loss_train: 1.0984 acc_train: 0.7000 loss_val: 1.3543 acc_val: 0.5907 time: 0.0100s\n",
            "Epoch: 0070 loss_train: 1.0427 acc_train: 0.7357 loss_val: 1.3450 acc_val: 0.6016 time: 0.0120s\n",
            "Epoch: 0071 loss_train: 1.0526 acc_train: 0.7357 loss_val: 1.3361 acc_val: 0.6090 time: 0.0109s\n",
            "Epoch: 0072 loss_train: 1.0155 acc_train: 0.7714 loss_val: 1.3274 acc_val: 0.6184 time: 0.0097s\n",
            "Epoch: 0073 loss_train: 1.0032 acc_train: 0.7571 loss_val: 1.3188 acc_val: 0.6262 time: 0.0097s\n",
            "Epoch: 0074 loss_train: 0.9818 acc_train: 0.7429 loss_val: 1.3103 acc_val: 0.6340 time: 0.0098s\n",
            "Epoch: 0075 loss_train: 0.9977 acc_train: 0.7571 loss_val: 1.3022 acc_val: 0.6425 time: 0.0127s\n",
            "Epoch: 0076 loss_train: 0.9597 acc_train: 0.7929 loss_val: 1.2942 acc_val: 0.6460 time: 0.0107s\n",
            "Epoch: 0077 loss_train: 0.9543 acc_train: 0.7929 loss_val: 1.2862 acc_val: 0.6476 time: 0.0102s\n",
            "Epoch: 0078 loss_train: 0.9508 acc_train: 0.8000 loss_val: 1.2783 acc_val: 0.6464 time: 0.0098s\n",
            "Epoch: 0079 loss_train: 0.9401 acc_train: 0.7929 loss_val: 1.2700 acc_val: 0.6452 time: 0.0106s\n",
            "Epoch: 0080 loss_train: 0.9238 acc_train: 0.7786 loss_val: 1.2616 acc_val: 0.6491 time: 0.0100s\n",
            "Epoch: 0081 loss_train: 0.9323 acc_train: 0.8000 loss_val: 1.2529 acc_val: 0.6550 time: 0.0101s\n",
            "Epoch: 0082 loss_train: 0.9179 acc_train: 0.8429 loss_val: 1.2445 acc_val: 0.6604 time: 0.0105s\n",
            "Epoch: 0083 loss_train: 0.9026 acc_train: 0.8000 loss_val: 1.2362 acc_val: 0.6686 time: 0.0107s\n",
            "Epoch: 0084 loss_train: 0.9099 acc_train: 0.8071 loss_val: 1.2280 acc_val: 0.6783 time: 0.0105s\n",
            "Epoch: 0085 loss_train: 0.8566 acc_train: 0.8429 loss_val: 1.2205 acc_val: 0.6889 time: 0.0124s\n",
            "Epoch: 0086 loss_train: 0.8653 acc_train: 0.8214 loss_val: 1.2136 acc_val: 0.6978 time: 0.0075s\n",
            "Epoch: 0087 loss_train: 0.8580 acc_train: 0.8643 loss_val: 1.2072 acc_val: 0.7048 time: 0.0091s\n",
            "Epoch: 0088 loss_train: 0.8595 acc_train: 0.8429 loss_val: 1.2011 acc_val: 0.7114 time: 0.0151s\n",
            "Epoch: 0089 loss_train: 0.8718 acc_train: 0.8500 loss_val: 1.1947 acc_val: 0.7138 time: 0.0087s\n",
            "Epoch: 0090 loss_train: 0.8456 acc_train: 0.8571 loss_val: 1.1881 acc_val: 0.7134 time: 0.0089s\n",
            "Epoch: 0091 loss_train: 0.8383 acc_train: 0.8786 loss_val: 1.1816 acc_val: 0.7122 time: 0.0098s\n",
            "Epoch: 0092 loss_train: 0.7876 acc_train: 0.8643 loss_val: 1.1753 acc_val: 0.7126 time: 0.0102s\n",
            "Epoch: 0093 loss_train: 0.7990 acc_train: 0.8643 loss_val: 1.1690 acc_val: 0.7157 time: 0.0107s\n",
            "Epoch: 0094 loss_train: 0.7911 acc_train: 0.8929 loss_val: 1.1620 acc_val: 0.7126 time: 0.0120s\n",
            "Epoch: 0095 loss_train: 0.7612 acc_train: 0.8643 loss_val: 1.1551 acc_val: 0.7122 time: 0.0106s\n",
            "Epoch: 0096 loss_train: 0.8129 acc_train: 0.8429 loss_val: 1.1487 acc_val: 0.7138 time: 0.0091s\n",
            "Epoch: 0097 loss_train: 0.7953 acc_train: 0.8643 loss_val: 1.1428 acc_val: 0.7157 time: 0.0107s\n",
            "Epoch: 0098 loss_train: 0.7453 acc_train: 0.8857 loss_val: 1.1372 acc_val: 0.7192 time: 0.0103s\n",
            "Epoch: 0099 loss_train: 0.8120 acc_train: 0.8643 loss_val: 1.1317 acc_val: 0.7235 time: 0.0103s\n",
            "Epoch: 0100 loss_train: 0.7274 acc_train: 0.8571 loss_val: 1.1267 acc_val: 0.7301 time: 0.0098s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 1.1299s\n",
            "Test set results: loss= 1.1267 accuracy= 0.7301\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "t_total = time.time()\n",
        "for epoch in range(args[\"epochs\"]):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# evaluating\n",
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF3eM6DhHfE_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCFwzVLmPXnH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Attention Networks"
      ],
      "metadata": {
        "id": "mKHEyXp1EVdo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx15HdotKnt_"
      },
      "source": [
        "## Graph attention layer (Your task)\n",
        "A GAT is made up of multiple such layers. In this section, you will implement a single graph attention layer. Similar to the `GraphConvolution()`, this `GraphAttentionLayer()` module takes $\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$ where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input and outputs $\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$, where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$. However, instead of weighing each neighbouring node based on the adjacency matrix, we will use self attention to learn the relative importance of each neighbouring node. Recall from HW4 where you are asked to write out the equation for single headed attention, here we will implement multi-headed attention, which involves the following steps:\n",
        "\n",
        "\n",
        "### The initial transformation\n",
        "In GCN above, you have completed similar transformation. But here, we need to define a weight matrix and perform this transformation for each head: $\\overrightarrow{s^k_i} = \\mathbf{W}^k \\overrightarrow{h_i}$. We will perform a single linear transformation and then split it up for each head later. Note the input $\\overrightarrow{h}$ has shape `[n_nodes, in_features]` and $\\overrightarrow{s}$ has shape of `[n_nodes, n_heads * n_hidden]`. Remember to reshape $\\overrightarrow{s}$ has shape of `[n_nodes, n_heads, n_hidden]` for later uses. Note: set `bias=False` for this linear transformation.\n",
        "\n",
        "### attention score\n",
        "We calculate these for each head $k$. Here for simplicity of the notation, we omit $k$ in the following equations. The attention scores are defined as the follows:\n",
        "$$e_{ij} = a(\\mathbf{W} \\overrightarrow{h_i}, \\mathbf{W} \\overrightarrow{h_j}) =a(\\overrightarrow{s_i}, \\overrightarrow{s_j})$$,\n",
        "where $e_{ij}$ is the attention score (importance) of node $j$ to node $i$.\n",
        "We will have to calculate this for each head. $a$ is the attention mechanism, that calculates the attention score. The paper concatenates $\\overrightarrow{s_i}$, $\\overrightarrow{s_j}$ and does a linear transformation with a weight vector $\\mathbf{a} \\in \\mathbb{R}^{2 F'}$ followed by a $\\text{LeakyReLU}$. $$e_{ij} = \\text{LeakyReLU} \\Big(\n",
        "\\mathbf{a}^\\top \\Big[ \\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}  \\Big] \\Big)$$\n",
        "\n",
        "#### How to vectorize this? Some hints:\n",
        "1. `tensor.repeat()` gives you $\\{\\overrightarrow{s_1}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_N}, \\overrightarrow{s_1}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_N}, ...\\}$.\n",
        "\n",
        "2. `tensor.repeat_interleave()` gives you\n",
        "$\\{\\overrightarrow{s_1}, \\overrightarrow{s_1}, \\dots, \\overrightarrow{s_1}, \\overrightarrow{s_2}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_2}, ...\\}$.\n",
        "\n",
        "3. concatenate to get $\\Big[\\overrightarrow{s_i} \\Vert \\overrightarrow{s_j} \\Big]$ for all pairs of $i, j$. Reshape $\\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}$ has shape of `[n_nodes, n_nodes, n_heads, 2 * n_hidden]`\n",
        "\n",
        "4. apply the attention layer and non-linear activation function to get $e_{ij} = \\text{LeakyReLU} \\Big( \\mathbf{a}^\\top \\Big[ \\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}  \\Big] \\Big)$, where $\\mathbf{a}^\\top$ is a single linear transformation that maps from dimension `n_hidden * 2` to `1`. Note: set the `bias=False` for this linear transformation. $\\mathbf{e}$ is of shape `[n_nodes, n_nodes, n_heads, 1]`. Remove the last dimension `1` using `squeeze()`.\n",
        "\n",
        "\n",
        "#### Perform softmax\n",
        "First, we need to mask $e_{ij}$ based on adjacency matrix. We only need to sum over the neighbouring nodes for the attention calculation. Set the elements in $e_{ij}$ to $- \\infty$ if there is no edge from $i$ to $j$ for the softmax calculation. We need to do this for all heads and the adjacency matrix is the same for each head. Use `tensor.masked_fill()` to mask $e_{ij}$ based on adjacency matrix for all heads. Hint: reshape the adjacency matrix to `[n_nodes, n_nodes, 1]` using `unsqueeze()`.\n",
        "Now we are ready to normalize attention scores (or coefficients) $$\\alpha_{ij} = \\text{softmax}_j(e_{ij}) =  \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}$$\n",
        "\n",
        "#### Apply dropout\n",
        "Apply the dropout layer. (this step is easy)\n",
        "\n",
        "#### Calculate final output for each head\n",
        "$$\\overrightarrow{h'^k_i} = \\sum_{j \\in \\mathcal{N}_i} \\alpha^k_{ij} \\overrightarrow{s^k_j}$$\n",
        "\n",
        "\n",
        "#### Concat or Mean\n",
        "Finally we concateneate the transformed features: $\\overrightarrow{h'_i} = \\Bigg\\Vert_{k=1}^{K} \\overrightarrow{h'^k_i}$. In the code, we only need to reshape the tensor to shape of `[n_nodes, n_heads * n_hidden]`. Note that if it is the final layer, then it doesn't make sense to do concatenation anymore. Instead, we sum over the `n_heads` dimension: $\\overrightarrow{h'_i} = \\frac{1}{K} \\sum_{k=1}^{K} \\overrightarrow{h'^k_i}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVu7rcOuAUZz"
      },
      "outputs": [],
      "source": [
        "class GraphAttentionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features: int, out_features: int, n_heads: int,\n",
        "                 is_concat: bool = True,\n",
        "                 dropout: float = 0.6,\n",
        "                 alpha: float = 0.2):\n",
        "        \"\"\"\n",
        "        in_features: F, the number of input features per node\n",
        "        out_features: F', the number of output features per node\n",
        "        n_heads: K, the number of attention heads\n",
        "        is_concat: whether the multi-head results should be concatenated or averaged\n",
        "        dropout: the dropout probability\n",
        "        alpha: the negative slope for leaky relu activation\n",
        "        \"\"\"\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "\n",
        "        self.is_concat = is_concat\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        if is_concat:\n",
        "            assert out_features % n_heads == 0\n",
        "            self.n_hidden = out_features // n_heads\n",
        "        else:\n",
        "            self.n_hidden = out_features\n",
        "\n",
        "        # TODO: initialize the following modules:\n",
        "        # (1) self.W: Linear layer that transform the input feature before self attention.\n",
        "        # You should NOT use for loops for the multiheaded implementation (set bias = Flase)\n",
        "        # (2) self.attention: Linear layer that compute the attention score (set bias = Flase)\n",
        "        # (3) self.activation: Activation function (LeakyReLU whith negative_slope=alpha)\n",
        "        # (4) self.softmax: Softmax function (what's the dim to compute the summation?)\n",
        "        # (5) self.dropout_layer: Dropout function(with ratio=dropout)\n",
        "        ################ your code here ########################\n",
        "        self.W = nn.Linear(in_features=in_features, out_features=out_features, bias=False)\n",
        "        self.attention = nn.Linear(in_features=self.n_hidden*2, out_features=1, bias=False)\n",
        "        self.activation = nn.LeakyReLU(negative_slope=alpha)\n",
        "        self.softmax = nn.Softmax(dim=0)\n",
        "        self.dropout_layer = nn.Dropout(p=dropout)\n",
        "        ########################################################\n",
        "\n",
        "    def forward(self, h: torch.Tensor, adj_mat: torch.Tensor):\n",
        "        # Number of nodes\n",
        "        n_nodes = h.shape[0]\n",
        "\n",
        "        # TODO:\n",
        "        # (1) calculate s = Wh and reshape it to [n_nodes, n_heads, n_hidden]\n",
        "        #     (you can use tensor.view() function)\n",
        "        # (2) get [s_i || s_j] using tensor.repeat(), repeat_interleave(), torch.cat(), tensor.view()\n",
        "        # (3) apply the attention layer\n",
        "        # (4) apply the activation layer (you will get the attention score e)\n",
        "        # (5) remove the last dimension 1 use tensor.squeeze()\n",
        "        # (6) mask the attention score with the adjacency matrix (if there's no edge, assign it to -inf)\n",
        "        #     note: check the dimensions of e and your adjacency matrix. You may need to use the function unsqueeze()\n",
        "        # (7) apply softmax\n",
        "        # (8) apply dropout_layer\n",
        "        ############## Your code here #########################################\n",
        "        s = self.W(h).view(n_nodes, self.n_heads, self.n_hidden)\n",
        "        s_repeat = s.repeat(n_nodes, 1, 1)\n",
        "        s_interleave = s.repeat_interleave(n_nodes, dim=0)\n",
        "        s_concat = torch.concat([s_repeat, s_interleave], dim=-1)\n",
        "        s_out = s_concat.view(n_nodes, n_nodes, self.n_heads, 2 * self.n_hidden)\n",
        "        out = self.activation(self.attention(s_out)) # n_nodes, n_nodes, n_heads, 1\n",
        "        out = out.squeeze(-1)\n",
        "        adj_mat = adj_mat.unsqueeze(-1).repeat(1, 1, 8)\n",
        "        print(out.shape, adj_mat.shape)\n",
        "\n",
        "        out = out.masked_fill(adj_mat == 0, float('-inf'))\n",
        "        a = self.dropout_layer(self.softmax(out))\n",
        "        #######################################################################\n",
        "\n",
        "        # Summation\n",
        "        h_prime = torch.einsum('ijh,jhf->ihf', a, s) #[n_nodes, n_heads, n_hidden]\n",
        "\n",
        "\n",
        "        # TODO: Concat or Mean\n",
        "        # Concatenate the heads\n",
        "        if self.is_concat:\n",
        "            ############## Your code here #########################################\n",
        "            return h_prime.reshape(n_nodes, self.n_heads * self.n_hidden)\n",
        "\n",
        "            #######################################################################\n",
        "        # Take the mean of the heads (for the last layer)\n",
        "        else:\n",
        "            ############## Your code here #########################################\n",
        "\n",
        "            return h_prime.mean(dim=1)\n",
        "\n",
        "            #######################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define GAT network\n",
        "it's really similar to how we defined GCN. We followed the paper to use two attention layers and ELU() activation function."
      ],
      "metadata": {
        "id": "YOSk_ZShi2nR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAT(nn.Module):\n",
        "\n",
        "    def __init__(self, nfeat: int, n_hidden: int, n_classes: int, n_heads: int, dropout: float, alpha: float):\n",
        "        \"\"\"\n",
        "        in_features: the number of features per node\n",
        "        n_hidden: the number of features in the first graph attention layer\n",
        "        n_classes: the number of classes\n",
        "        n_heads: the number of heads in the graph attention layers\n",
        "        dropout: the dropout probability\n",
        "        alpha: the negative input slope for leaky ReLU of the attention layer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # First graph attention layer where we concatenate the heads\n",
        "        self.gc1 = GraphAttentionLayer(nfeat, n_hidden, n_heads, is_concat=True, dropout=dropout, alpha=alpha)\n",
        "        self.gc2 = GraphAttentionLayer(n_hidden, n_classes, 1, is_concat=False, dropout=dropout, alpha=alpha)\n",
        "        self.activation = nn.LeakyReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, adj_mat: torch.Tensor):\n",
        "        \"\"\"\n",
        "        x: the features vectors\n",
        "        adj_mat: the adjacency matrix\n",
        "        \"\"\"\n",
        "        x = self.dropout(x)\n",
        "        x = self.gc1(x, adj_mat)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.gc2(x, adj_mat)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jKNbUtPVi1Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training GAT"
      ],
      "metadata": {
        "id": "CtRQ3Ced7RAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\"training_samples\": 140,\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 0.01,\n",
        "        \"weight_decay\": 5e-4,\n",
        "        \"hidden\": 16,\n",
        "        \"dropout\": 0.5,\n",
        "        \"bias\": True,\n",
        "        \"alpha\": 0.2,\n",
        "        \"n_heads\": 8\n",
        "        }"
      ],
      "metadata": {
        "id": "b7D5mYXC6zTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MYaK98hDy7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1609488-4bda-4103-e3d4-7b0178bfae53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n"
          ]
        }
      ],
      "source": [
        "model = GAT(nfeat=features.shape[1],\n",
        "            n_hidden=args[\"hidden\"],\n",
        "            n_classes=labels.max().item() + 1,\n",
        "            dropout=args[\"dropout\"],\n",
        "            alpha=args[\"alpha\"],\n",
        "            n_heads=args[\"n_heads\"]).to(device)\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n",
        "\n",
        "adj, features, labels, idx_train, idx_val = load_data(training_samples=args[\"training_samples\"])\n",
        "adj, features, labels, idx_train, idx_val = adj.to(device), features.to(device), labels.to(device), idx_train.to(device), idx_val.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9FcfXwMDzEt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63bb7096-f575-4069-8b36-8b9498cfb2c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0001 loss_train: 1.9459 acc_train: 0.1500 loss_val: 1.9441 acc_val: 0.3065 time: 1.0077s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0002 loss_train: 1.9429 acc_train: 0.3857 loss_val: 1.9418 acc_val: 0.3033 time: 0.9065s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0003 loss_train: 1.9393 acc_train: 0.3714 loss_val: 1.9389 acc_val: 0.3018 time: 0.9149s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0004 loss_train: 1.9348 acc_train: 0.3571 loss_val: 1.9358 acc_val: 0.3010 time: 0.9143s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0005 loss_train: 1.9282 acc_train: 0.3714 loss_val: 1.9323 acc_val: 0.3006 time: 0.9170s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0006 loss_train: 1.9209 acc_train: 0.3643 loss_val: 1.9284 acc_val: 0.3002 time: 0.9091s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0007 loss_train: 1.9151 acc_train: 0.3500 loss_val: 1.9241 acc_val: 0.2998 time: 0.9171s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0008 loss_train: 1.9085 acc_train: 0.3571 loss_val: 1.9195 acc_val: 0.2998 time: 0.9159s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0009 loss_train: 1.8995 acc_train: 0.3643 loss_val: 1.9146 acc_val: 0.2998 time: 0.9154s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0010 loss_train: 1.8925 acc_train: 0.3643 loss_val: 1.9095 acc_val: 0.2998 time: 0.9069s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0011 loss_train: 1.8844 acc_train: 0.3571 loss_val: 1.9041 acc_val: 0.2998 time: 0.9146s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0012 loss_train: 1.8716 acc_train: 0.3643 loss_val: 1.8985 acc_val: 0.2998 time: 0.9168s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0013 loss_train: 1.8669 acc_train: 0.3643 loss_val: 1.8927 acc_val: 0.2998 time: 0.9175s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0014 loss_train: 1.8467 acc_train: 0.3357 loss_val: 1.8867 acc_val: 0.2998 time: 0.9067s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0015 loss_train: 1.8387 acc_train: 0.3500 loss_val: 1.8804 acc_val: 0.2998 time: 0.9151s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0016 loss_train: 1.8269 acc_train: 0.3500 loss_val: 1.8740 acc_val: 0.2998 time: 0.9173s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0017 loss_train: 1.8168 acc_train: 0.3500 loss_val: 1.8675 acc_val: 0.2998 time: 0.9163s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0018 loss_train: 1.8018 acc_train: 0.3429 loss_val: 1.8608 acc_val: 0.2998 time: 0.9093s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0019 loss_train: 1.7974 acc_train: 0.3571 loss_val: 1.8541 acc_val: 0.2998 time: 0.9151s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0020 loss_train: 1.7779 acc_train: 0.3571 loss_val: 1.8473 acc_val: 0.2998 time: 0.9167s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0021 loss_train: 1.7686 acc_train: 0.3500 loss_val: 1.8405 acc_val: 0.2998 time: 0.9164s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0022 loss_train: 1.7565 acc_train: 0.3571 loss_val: 1.8336 acc_val: 0.2998 time: 0.9071s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0023 loss_train: 1.7395 acc_train: 0.3571 loss_val: 1.8267 acc_val: 0.2998 time: 0.9158s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0024 loss_train: 1.7234 acc_train: 0.3571 loss_val: 1.8196 acc_val: 0.2998 time: 0.9144s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0025 loss_train: 1.7138 acc_train: 0.3500 loss_val: 1.8124 acc_val: 0.2998 time: 0.9177s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0026 loss_train: 1.6799 acc_train: 0.3571 loss_val: 1.8051 acc_val: 0.2998 time: 0.9065s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0027 loss_train: 1.6838 acc_train: 0.3643 loss_val: 1.7977 acc_val: 0.2995 time: 0.9188s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0028 loss_train: 1.6858 acc_train: 0.3571 loss_val: 1.7901 acc_val: 0.2995 time: 0.9203s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0029 loss_train: 1.6528 acc_train: 0.3500 loss_val: 1.7824 acc_val: 0.2995 time: 0.9258s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0030 loss_train: 1.6333 acc_train: 0.3643 loss_val: 1.7745 acc_val: 0.2998 time: 0.9124s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0031 loss_train: 1.6222 acc_train: 0.3643 loss_val: 1.7664 acc_val: 0.2998 time: 0.9187s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0032 loss_train: 1.6051 acc_train: 0.3500 loss_val: 1.7580 acc_val: 0.3002 time: 0.9223s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0033 loss_train: 1.5961 acc_train: 0.3643 loss_val: 1.7496 acc_val: 0.3006 time: 0.9252s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0034 loss_train: 1.5919 acc_train: 0.3643 loss_val: 1.7411 acc_val: 0.3010 time: 0.9077s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0035 loss_train: 1.5680 acc_train: 0.3571 loss_val: 1.7325 acc_val: 0.3010 time: 0.9219s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0036 loss_train: 1.5514 acc_train: 0.3786 loss_val: 1.7239 acc_val: 0.3033 time: 0.9227s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0037 loss_train: 1.5342 acc_train: 0.3571 loss_val: 1.7152 acc_val: 0.3045 time: 0.9220s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0038 loss_train: 1.4926 acc_train: 0.3714 loss_val: 1.7063 acc_val: 0.3061 time: 0.9089s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0039 loss_train: 1.5323 acc_train: 0.4000 loss_val: 1.6972 acc_val: 0.3084 time: 0.9210s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0040 loss_train: 1.4849 acc_train: 0.3929 loss_val: 1.6878 acc_val: 0.3115 time: 0.9178s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0041 loss_train: 1.4913 acc_train: 0.4357 loss_val: 1.6783 acc_val: 0.3154 time: 0.9223s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0042 loss_train: 1.4627 acc_train: 0.4643 loss_val: 1.6686 acc_val: 0.3232 time: 0.9080s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0043 loss_train: 1.4531 acc_train: 0.4500 loss_val: 1.6587 acc_val: 0.3279 time: 0.9134s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0044 loss_train: 1.4499 acc_train: 0.4143 loss_val: 1.6488 acc_val: 0.3357 time: 0.9158s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0045 loss_train: 1.4629 acc_train: 0.4500 loss_val: 1.6387 acc_val: 0.3411 time: 0.9172s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0046 loss_train: 1.4100 acc_train: 0.4429 loss_val: 1.6284 acc_val: 0.3485 time: 0.9074s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0047 loss_train: 1.4492 acc_train: 0.4643 loss_val: 1.6179 acc_val: 0.3575 time: 0.9145s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0048 loss_train: 1.4139 acc_train: 0.4857 loss_val: 1.6072 acc_val: 0.3633 time: 0.9153s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0049 loss_train: 1.3855 acc_train: 0.5214 loss_val: 1.5967 acc_val: 0.3754 time: 0.9166s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0050 loss_train: 1.4059 acc_train: 0.4643 loss_val: 1.5863 acc_val: 0.3898 time: 0.9068s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0051 loss_train: 1.4058 acc_train: 0.5143 loss_val: 1.5759 acc_val: 0.3995 time: 0.9138s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0052 loss_train: 1.3416 acc_train: 0.5214 loss_val: 1.5654 acc_val: 0.4112 time: 0.9143s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0053 loss_train: 1.3641 acc_train: 0.5214 loss_val: 1.5550 acc_val: 0.4225 time: 0.9162s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0054 loss_train: 1.3277 acc_train: 0.5214 loss_val: 1.5449 acc_val: 0.4319 time: 0.9122s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0055 loss_train: 1.3107 acc_train: 0.5929 loss_val: 1.5348 acc_val: 0.4416 time: 0.9189s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0056 loss_train: 1.2902 acc_train: 0.5857 loss_val: 1.5249 acc_val: 0.4517 time: 0.9149s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0057 loss_train: 1.2809 acc_train: 0.6214 loss_val: 1.5149 acc_val: 0.4618 time: 0.9175s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0058 loss_train: 1.2916 acc_train: 0.5643 loss_val: 1.5052 acc_val: 0.4704 time: 0.9062s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0059 loss_train: 1.2722 acc_train: 0.6429 loss_val: 1.4956 acc_val: 0.4794 time: 0.9150s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0060 loss_train: 1.2343 acc_train: 0.6286 loss_val: 1.4862 acc_val: 0.4875 time: 0.9165s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0061 loss_train: 1.2621 acc_train: 0.5857 loss_val: 1.4770 acc_val: 0.4914 time: 0.9132s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0062 loss_train: 1.2504 acc_train: 0.6714 loss_val: 1.4680 acc_val: 0.4977 time: 0.9063s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0063 loss_train: 1.1942 acc_train: 0.6500 loss_val: 1.4589 acc_val: 0.5000 time: 0.9157s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0064 loss_train: 1.1952 acc_train: 0.6286 loss_val: 1.4500 acc_val: 0.5058 time: 0.9160s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0065 loss_train: 1.2057 acc_train: 0.6714 loss_val: 1.4414 acc_val: 0.5109 time: 0.9158s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0066 loss_train: 1.2414 acc_train: 0.6286 loss_val: 1.4330 acc_val: 0.5171 time: 0.9079s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0067 loss_train: 1.1704 acc_train: 0.6357 loss_val: 1.4247 acc_val: 0.5238 time: 0.9159s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0068 loss_train: 1.2077 acc_train: 0.6357 loss_val: 1.4164 acc_val: 0.5292 time: 0.9166s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0069 loss_train: 1.1845 acc_train: 0.6857 loss_val: 1.4079 acc_val: 0.5374 time: 0.9166s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0070 loss_train: 1.1265 acc_train: 0.6857 loss_val: 1.3993 acc_val: 0.5460 time: 0.9038s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0071 loss_train: 1.1567 acc_train: 0.6929 loss_val: 1.3909 acc_val: 0.5530 time: 0.9133s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0072 loss_train: 1.1614 acc_train: 0.6786 loss_val: 1.3827 acc_val: 0.5580 time: 0.9153s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0073 loss_train: 1.1430 acc_train: 0.6714 loss_val: 1.3747 acc_val: 0.5615 time: 0.9164s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0074 loss_train: 1.0873 acc_train: 0.7500 loss_val: 1.3669 acc_val: 0.5650 time: 0.9070s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0075 loss_train: 1.0946 acc_train: 0.7000 loss_val: 1.3593 acc_val: 0.5744 time: 0.9196s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0076 loss_train: 1.1258 acc_train: 0.7143 loss_val: 1.3519 acc_val: 0.5798 time: 0.9169s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0077 loss_train: 1.0908 acc_train: 0.7214 loss_val: 1.3443 acc_val: 0.5829 time: 0.9174s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0078 loss_train: 1.1142 acc_train: 0.6929 loss_val: 1.3371 acc_val: 0.5876 time: 0.9056s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0079 loss_train: 1.0390 acc_train: 0.6929 loss_val: 1.3299 acc_val: 0.5942 time: 0.9149s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0080 loss_train: 1.0953 acc_train: 0.6857 loss_val: 1.3227 acc_val: 0.5985 time: 0.9136s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0081 loss_train: 1.0411 acc_train: 0.7071 loss_val: 1.3157 acc_val: 0.6063 time: 0.9150s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0082 loss_train: 1.0229 acc_train: 0.7500 loss_val: 1.3088 acc_val: 0.6079 time: 0.9036s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0083 loss_train: 1.0120 acc_train: 0.7571 loss_val: 1.3022 acc_val: 0.6118 time: 0.9167s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0084 loss_train: 1.0125 acc_train: 0.7357 loss_val: 1.2956 acc_val: 0.6153 time: 0.9183s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0085 loss_train: 1.0412 acc_train: 0.6929 loss_val: 1.2892 acc_val: 0.6199 time: 0.9182s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0086 loss_train: 1.0560 acc_train: 0.7143 loss_val: 1.2831 acc_val: 0.6231 time: 0.9085s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0087 loss_train: 0.9839 acc_train: 0.7286 loss_val: 1.2770 acc_val: 0.6258 time: 0.9150s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0088 loss_train: 0.9777 acc_train: 0.7286 loss_val: 1.2711 acc_val: 0.6285 time: 0.9149s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0089 loss_train: 0.9763 acc_train: 0.7786 loss_val: 1.2650 acc_val: 0.6343 time: 0.9127s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0090 loss_train: 1.0122 acc_train: 0.6929 loss_val: 1.2588 acc_val: 0.6386 time: 0.9069s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0091 loss_train: 1.0069 acc_train: 0.7143 loss_val: 1.2526 acc_val: 0.6417 time: 0.9135s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0092 loss_train: 0.9556 acc_train: 0.7571 loss_val: 1.2465 acc_val: 0.6441 time: 0.9171s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0093 loss_train: 0.9971 acc_train: 0.7857 loss_val: 1.2408 acc_val: 0.6449 time: 0.9167s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0094 loss_train: 0.9729 acc_train: 0.7214 loss_val: 1.2353 acc_val: 0.6472 time: 0.9086s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0095 loss_train: 0.9098 acc_train: 0.7357 loss_val: 1.2298 acc_val: 0.6491 time: 0.9137s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0096 loss_train: 0.9626 acc_train: 0.7143 loss_val: 1.2245 acc_val: 0.6511 time: 0.9144s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0097 loss_train: 0.9202 acc_train: 0.7857 loss_val: 1.2192 acc_val: 0.6511 time: 0.9149s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0098 loss_train: 0.9913 acc_train: 0.7143 loss_val: 1.2145 acc_val: 0.6511 time: 0.9048s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0099 loss_train: 0.9009 acc_train: 0.7500 loss_val: 1.2100 acc_val: 0.6503 time: 0.9112s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Epoch: 0100 loss_train: 0.9318 acc_train: 0.7571 loss_val: 1.2056 acc_val: 0.6476 time: 0.9134s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 91.6451s\n",
            "torch.Size([2708, 2708, 8]) torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708, 1]) torch.Size([2708, 2708, 8])\n",
            "Test set results: loss= 1.2056 accuracy= 0.6476\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "t_total = time.time()\n",
        "for epoch in range(args[\"epochs\"]):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# Testing\n",
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "uRqcWJ4lO7Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question: (Your task)\n",
        "Compare the evaluation results for Vanilla GCN and GAT. Comment on the discrepancy in their performance (if any) and briefly explain why you think it's the case (in 1-2 sentences)."
      ],
      "metadata": {
        "id": "n6Ox3fbTG7rc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The validation accuracy at the 100th iteration of the Vanilla GCN is higher than that of GAT. In fact, the validation accuracy start decrease at the 97th epoch for the GAT while it was always increasing for GCN (up till the 100th epochs). This might be caused by with more parameters introduced to the model, GAT might need more epochs to train."
      ],
      "metadata": {
        "id": "fc3-VaeYWbHR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZhHh8k4DzJu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmvJ46OfGlf2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}