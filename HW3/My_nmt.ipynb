{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjPTaRB4mpCd"
      },
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignment by selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9IS9B9-yUU5"
      },
      "source": [
        "# Setup PyTorch\n",
        "\n",
        "All files will be stored at /content/csc421/a3/ folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-6MQhMOlHXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05af3b43-75b4-4411-b2a7-8f3ca5513baa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "/content/content/csc421/a3\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install Pillow\n",
        "%mkdir -p ./content/csc421/a3/\n",
        "%cd ./content/csc421/a3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DaTdRNuUra7"
      },
      "source": [
        "# Helper code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIpGwANoQOg"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-UJHBYZkh7f"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "from pathlib import Path\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "\n",
        "def get_file(\n",
        "    fname, origin, untar=False, extract=False, archive_format=\"auto\", cache_dir=\"data\"\n",
        "):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + \".tar.gz\"\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "\n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print(\"Downloading data from\", origin)\n",
        "\n",
        "        error_msg = \"URL fetch failure on {}: {} -- {}\"\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print(\"Extracting file.\")\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "\n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "    Arguments:\n",
        "        tensor: A Tensor object.\n",
        "        cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "    Returns:\n",
        "        A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title(\"BS={}, nhid={}\".format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel(\"Epochs\", fontsize=16)\n",
        "    plt.ylabel(\"Loss\", fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, \"loss_plot.pdf\"))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_loss_comparison_gru(l1, l2, o1, o2, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and val loss curves from GRU runs.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    ax[0].plot(range(len(mean_l1)), mean_l1, label=\"ds=\" + o1.data_file_name)\n",
        "    ax[0].plot(range(len(mean_l2)), mean_l2, label=\"ds=\" + o2.data_file_name)\n",
        "    ax[0].title.set_text(\"Train Loss | GRU Hidden Size = {}\".format(o2.hidden_size))\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[1].plot(range(len(l1[1])), l1[1], label=\"ds=\" + o1.data_file_name)\n",
        "    ax[1].plot(range(len(l2[1])), l2[1], label=\"ds=\" + o2.data_file_name)\n",
        "    ax[1].title.set_text(\"Val Loss | GRU Hidden Size = {}\".format(o2.hidden_size))\n",
        "\n",
        "    ax[0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "    ax[0].set_ylabel(\"Loss\", fontsize=10)\n",
        "    ax[1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "    ax[1].set_ylabel(\"Loss\", fontsize=10)\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"GRU Performance by Dataset\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.85)\n",
        "    plt.legend()\n",
        "\n",
        "    plt_path = \"./loss_plot_{}.pdf\".format(fn)\n",
        "    plt.savefig(plt_path)\n",
        "    print(f\"Plot saved to: {Path(plt_path).resolve()}\")\n",
        "\n",
        "\n",
        "def save_loss_comparison_by_dataset(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
        "    runs in Part 3, comparing by dataset while holding hidden size constant.\n",
        "\n",
        "    Models within each pair (l1, l2) and (l3, l4) have the same hidden sizes.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        l3: Tuple of lists containing training / val losses for model 3.\n",
        "        l4: Tuple of lists containing training / val losses for model 4.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        o3: Options for model 3.\n",
        "        o4: Options for model 4.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n",
        "    mean_l3 = [np.mean(l3[0][i * s : (i + 1) * s]) for i in range(len(l3[0]) // s)]\n",
        "    mean_l4 = [np.mean(l4[0][i * s : (i + 1) * s]) for i in range(len(l4[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label=\"ds=\" + o1.data_file_name)\n",
        "    ax[0][0].plot(range(len(mean_l2)), mean_l2, label=\"ds=\" + o2.data_file_name)\n",
        "    ax[0][0].title.set_text(\n",
        "        \"Train Loss | Model Hidden Size = {}\".format(o1.hidden_size)\n",
        "    )\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[0][1].plot(range(len(l1[1])), l1[1], label=\"ds=\" + o1.data_file_name)\n",
        "    ax[0][1].plot(range(len(l2[1])), l2[1], label=\"ds=\" + o2.data_file_name)\n",
        "    ax[0][1].title.set_text(\"Val Loss | Model Hidden Size = {}\".format(o1.hidden_size))\n",
        "\n",
        "    ax[1][0].plot(range(len(mean_l3)), mean_l3, label=\"ds=\" + o3.data_file_name)\n",
        "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label=\"ds=\" + o4.data_file_name)\n",
        "    ax[1][0].title.set_text(\n",
        "        \"Train Loss | Model Hidden Size = {}\".format(o3.hidden_size)\n",
        "    )\n",
        "\n",
        "    ax[1][1].plot(range(len(l3[1])), l3[1], label=\"ds=\" + o3.data_file_name)\n",
        "    ax[1][1].plot(range(len(l4[1])), l4[1], label=\"ds=\" + o4.data_file_name)\n",
        "    ax[1][1].title.set_text(\"Val Loss | Model Hidden Size = {}\".format(o4.hidden_size))\n",
        "\n",
        "    for i in range(2):\n",
        "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][0].legend(loc=\"upper right\")\n",
        "        ax[i][1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"Performance by Dataset Size\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.9)\n",
        "    plt.legend()\n",
        "    plt.savefig(\"./loss_plot_{}.pdf\".format(fn))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_loss_comparison_by_hidden(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
        "    runs in Part 3, comparing by hidden size while holding dataset constant.\n",
        "\n",
        "    Models within each pair (l1, l3) and (l2, l4) have the same dataset.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        l3: Tuple of lists containing training / val losses for model 3.\n",
        "        l4: Tuple of lists containing training / val losses for model 4.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        o3: Options for model 3.\n",
        "        o4: Options for model 4.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n",
        "    mean_l3 = [np.mean(l3[0][i * s : (i + 1) * s]) for i in range(len(l3[0]) // s)]\n",
        "    mean_l4 = [np.mean(l4[0][i * s : (i + 1) * s]) for i in range(len(l4[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label=\"hid_size=\" + str(o1.hidden_size))\n",
        "    ax[0][0].plot(range(len(mean_l3)), mean_l3, label=\"hid_size=\" + str(o3.hidden_size))\n",
        "    ax[0][0].title.set_text(\"Train Loss | Dataset = \" + o1.data_file_name)\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[0][1].plot(range(len(l1[1])), l1[1], label=\"hid_size=\" + str(o1.hidden_size))\n",
        "    ax[0][1].plot(range(len(l3[1])), l3[1], label=\"hid_size=\" + str(o3.hidden_size))\n",
        "    ax[0][1].title.set_text(\"Val Loss | Dataset = \" + o1.data_file_name)\n",
        "\n",
        "    ax[1][0].plot(range(len(mean_l2)), mean_l2, label=\"hid_size=\" + str(o2.hidden_size))\n",
        "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label=\"hid_size=\" + str(o4.hidden_size))\n",
        "    ax[1][0].title.set_text(\"Train Loss | Dataset = \" + o3.data_file_name)\n",
        "\n",
        "    ax[1][1].plot(range(len(l2[1])), l2[1], label=\"hid_size=\" + str(o2.hidden_size))\n",
        "    ax[1][1].plot(range(len(l4[1])), l4[1], label=\"hid_size=\" + str(o4.hidden_size))\n",
        "    ax[1][1].title.set_text(\"Val Loss | Dataset = \" + o4.data_file_name)\n",
        "\n",
        "    for i in range(2):\n",
        "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][0].legend(loc=\"upper right\")\n",
        "        ax[i][1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"Performance by Hidden State Size\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.9)\n",
        "    plt.legend()\n",
        "    plt.savefig(\"./loss_plot_{}.pdf\".format(fn))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, \"encoder.pt\"), \"wb\") as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, \"decoder.pt\"), \"wb\") as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, \"idx_dict.pkl\"), \"wb\") as f:\n",
        "        pkl.dump(idx_dict, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbvpn4MaV0I1"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVT4TNTOV3Eg"
      },
      "outputs": [],
      "source": [
        "def read_lines(filename):\n",
        "    \"\"\"Read a file and split it into lines.\"\"\"\n",
        "    lines = open(filename).read().strip().lower().split(\"\\n\")\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\"\"\"\n",
        "    return all(c.isalpha() or c == \"-\" for c in s)\n",
        "\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data(file_name):\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\"\"\"\n",
        "    path = \"./data/{}.txt\".format(file_name)\n",
        "    source_lines, target_lines = read_pairs(path)\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(\"\".join(source_lines)) | set(\"\".join(target_lines))\n",
        "\n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = {\n",
        "        char: index for (index, char) in enumerate(sorted(list(all_characters)))\n",
        "    }\n",
        "\n",
        "    # Add start and end tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    char_to_index[\"SOS\"] = start_token\n",
        "    char_to_index[\"EOS\"] = end_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = {index: char for (char, index) in char_to_index.items()}\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "\n",
        "    idx_dict = {\n",
        "        \"char_to_index\": char_to_index,\n",
        "        \"index_to_char\": index_to_char,\n",
        "        \"start_token\": start_token,\n",
        "        \"end_token\": end_token,\n",
        "    }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s, t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s, t))\n",
        "\n",
        "    return d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRWfRdmVVjUl"
      },
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wa5-onJhoSeM"
      },
      "outputs": [],
      "source": [
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\"\"\"\n",
        "    return [char_to_index[char] for char in s] + [\n",
        "        end_token\n",
        "    ]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "        line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
        "    return \" \".join(\n",
        "        [translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()]\n",
        "    )\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\"\"\"\n",
        "\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "    index_to_char = idx_dict[\"index_to_char\"]\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = \"\"\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(\n",
        "        torch.LongTensor(indexes).unsqueeze(0), opts.cuda\n",
        "    )  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(\n",
        "            decoder_inputs, encoder_annotations, decoder_hidden\n",
        "        )\n",
        "\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1]  # latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [\n",
        "                    index_to_char[int(item)]\n",
        "                    for item in generated_words.cpu().numpy().reshape(-1)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\"\"\"\n",
        "    if idx_dict is None:\n",
        "        line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "    index_to_char = idx_dict[\"index_to_char\"]\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = \"\"\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(\n",
        "        torch.LongTensor(indexes).unsqueeze(0), opts.cuda\n",
        "    )  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(\n",
        "            decoder_inputs, encoder_annotations, decoder_hidden\n",
        "        )\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1]  # latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [\n",
        "                    index_to_char[int(item)]\n",
        "                    for item in generated_words.cpu().numpy().reshape(-1)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    if isinstance(attention_weights, tuple):\n",
        "        ## transformer's attention mweights\n",
        "        attention_weights, self_attention_weights = attention_weights\n",
        "\n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "\n",
        "    for i in range(len(all_attention_weights)):\n",
        "        attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)\n",
        "        cax = ax.matshow(attention_weights_matrix, cmap=\"bone\")\n",
        "        fig.colorbar(cax)\n",
        "\n",
        "        # Set up axes\n",
        "        ax.set_yticklabels([\"\"] + list(input_string) + [\"EOS\"], rotation=90)\n",
        "        ax.set_xticklabels(\n",
        "            [\"\"] + list(gen_string) + ([\"EOS\"] if produced_end_token else [])\n",
        "        )\n",
        "\n",
        "        # Show label at every tick\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        # Add title\n",
        "        plt.xlabel(\"Attention weights to the source sentence in layer {}\".format(i + 1))\n",
        "        plt.tight_layout()\n",
        "        plt.grid(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model.\n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [\n",
        "            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n",
        "            for s in input_strings\n",
        "        ]\n",
        "        target_tensors = [\n",
        "            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n",
        "            for s in target_strings\n",
        "        ]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            start_vector = (\n",
        "                torch.ones(BS).long().unsqueeze(1) * start_token\n",
        "            )  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat(\n",
        "                [decoder_input, targets[:, 0:-1]], dim=1\n",
        "            )  # Gets decoder inputs by shifting the targets to the right\n",
        "\n",
        "            decoder_outputs, attention_weights = decoder(\n",
        "                decoder_inputs, encoder_annotations, decoder_hidden\n",
        "            )\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "\n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "                # Zero gradients\n",
        "                optimizer.zero_grad()\n",
        "                # Compute gradients\n",
        "                loss.backward()\n",
        "                # Update the parameters of the encoder and decoder\n",
        "                optimizer.step()\n",
        "\n",
        "    return losses\n",
        "\n",
        "\n",
        "def training_loop(\n",
        "    train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n",
        "):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "        * Returns loss curves for comparison\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        losses: Lists containing training and validation loss curves.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, \"loss_log.txt\"), \"w\")\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    mean_train_losses = []\n",
        "    mean_val_losses = []\n",
        "\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0][\"lr\"] *= opts.lr_decay\n",
        "\n",
        "        train_loss = compute_loss(\n",
        "            train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts\n",
        "        )\n",
        "        val_loss = compute_loss(\n",
        "            val_dict, encoder, decoder, idx_dict, criterion, None, opts\n",
        "        )\n",
        "\n",
        "        mean_train_loss = np.mean(train_loss)\n",
        "        mean_val_loss = np.mean(val_loss)\n",
        "\n",
        "        if mean_val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "            best_val_loss = mean_val_loss\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        if early_stopping_counter > opts.early_stopping_patience:\n",
        "            print(\n",
        "                \"Validation loss has not improved in {} epochs, stopping early\".format(\n",
        "                    opts.early_stopping_patience\n",
        "                )\n",
        "            )\n",
        "            print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "            return (train_losses, mean_val_losses)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\n",
        "            \"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(\n",
        "                epoch, mean_train_loss, mean_val_loss, gen_string\n",
        "            )\n",
        "        )\n",
        "\n",
        "        loss_log.write(\"{} {} {}\\n\".format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses += train_loss\n",
        "        val_losses += val_loss\n",
        "\n",
        "        mean_train_losses.append(mean_train_loss)\n",
        "        mean_val_losses.append(mean_val_loss)\n",
        "\n",
        "        save_loss_plot(mean_train_losses, mean_val_losses, opts)\n",
        "\n",
        "    print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "    return (train_losses, mean_val_losses)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Data Stats\".center(80))\n",
        "    print(\"-\" * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print(\"Num unique word pairs: {}\".format(len(line_pairs)))\n",
        "    print(\"Vocabulary: {}\".format(idx_dict[\"char_to_index\"].keys()))\n",
        "    print(\"Vocab size: {}\".format(vocab_size))\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    if opts.encoder_type == \"rnn\":\n",
        "        encoder = GRUEncoder(\n",
        "            vocab_size=vocab_size, hidden_size=opts.hidden_size, opts=opts\n",
        "        )\n",
        "    elif opts.encoder_type == \"transformer\":\n",
        "        encoder = TransformerEncoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "            num_layers=opts.num_transformer_layers,\n",
        "            opts=opts,\n",
        "        )\n",
        "    elif opts.encoder_type == \"attention\":\n",
        "      encoder = AttentionEncoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "            opts=opts,\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if opts.decoder_type == \"rnn\":\n",
        "        decoder = RNNDecoder(vocab_size=vocab_size, hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == \"rnn_attention\":\n",
        "        decoder = RNNAttentionDecoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "            attention_type=opts.attention_type,\n",
        "        )\n",
        "    elif opts.decoder_type == \"transformer\":\n",
        "        decoder = TransformerDecoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "            num_layers=opts.num_transformer_layers,\n",
        "        )\n",
        "    elif opts.encoder_type == \"attention\":\n",
        "      decoder = AttentionDecoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    #### setup checkpoint path\n",
        "    model_name = \"h{}-bs{}-{}-{}\".format(\n",
        "        opts.hidden_size, opts.batch_size, opts.decoder_type, opts.data_file_name\n",
        "    )\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(\n",
        "        list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        losses = training_loop(\n",
        "            train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n",
        "        )\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Exiting early from training.\")\n",
        "        return encoder, decoder, losses\n",
        "\n",
        "    return encoder, decoder, losses\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Opts\".center(80))\n",
        "    print(\"-\" * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print(\"{:>30}: {:<30}\".format(key, opts.__dict__[key]).center(80))\n",
        "    print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yh08KhgnA30"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aROU2xZanDKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "385b7a72-cc0f-41c1-9ba2-3af65228cc93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/pig_latin_small.txt\n",
            "Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_small.txt\n",
            "data/pig_latin_large.txt\n",
            "Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_large.txt\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(\n",
        "    fname=\"pig_latin_small.txt\",\n",
        "    origin=\"http://www.cs.toronto.edu/~jba/pig_latin_small.txt\",\n",
        "    untar=False,\n",
        ")\n",
        "\n",
        "data_fpath = get_file(\n",
        "    fname=\"pig_latin_large.txt\",\n",
        "    origin=\"http://www.cs.toronto.edu/~jba/pig_latin_large.txt\",\n",
        "    untar=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDYMr7NclZdw"
      },
      "source": [
        "# Part 1: Neural machine translation (NMT)\n",
        "\n",
        "In this section, you will implement a Gated Recurrent Unit (GRU) cell, a common type of recurrent neural network (RNN). The GRU cell is a simplification of the Long Short-Term Memory cell. Therefore, we have provided you with an implemented LSTM cell (`MyLSTMCell`), which you can reference when completing `MyGRUCell`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOnALRQkkjDO"
      },
      "outputs": [],
      "source": [
        "class MyLSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyLSTMCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Wif = nn.Linear(input_size, hidden_size)\n",
        "        self.Whf = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wii = nn.Linear(input_size, hidden_size)\n",
        "        self.Whi = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wic = nn.Linear(input_size, hidden_size)\n",
        "        self.Whc = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wio = nn.Linear(input_size, hidden_size)\n",
        "        self.Who = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, x, h_prev, c_prev):\n",
        "        \"\"\"Forward pass of the LSTM computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "            c_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "            c_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        f = torch.sigmoid(self.Wif(x) + self.Whf(h_prev))\n",
        "        i = torch.sigmoid(self.Wii(x) + self.Whi(h_prev))\n",
        "\n",
        "        c = torch.tanh(self.Wic(x) + self.Whc(h_prev))\n",
        "        o = torch.sigmoid(self.Wio(x) + self.Who(h_prev))\n",
        "\n",
        "        c_new = f * c_prev + i * c\n",
        "        h_new = o * torch.tanh(c_new)\n",
        "\n",
        "        return h_new, c_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCae1mOUlZrC"
      },
      "source": [
        "## Step 1: GRU Cell\n",
        "Please implement the `MyGRUCell` class defined in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGyxqZIQzTJH"
      },
      "outputs": [],
      "source": [
        "class MyGRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyGRUCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        # Input linear layers\n",
        "        self.Wiz = nn.Linear(input_size, hidden_size)\n",
        "        self.Wir = nn.Linear(input_size, hidden_size)\n",
        "        self.Wih = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        # Hidden linear layers\n",
        "        self.Whz = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Whr = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Whh = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"Forward pass of the GRU computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        z = torch.sigmoid(self.Wiz(x) + self.Whz(h_prev))\n",
        "        r = torch.sigmoid(self.Wir(x) + self.Whr(h_prev))\n",
        "        g = torch.tanh(self.Wih(x) + self.Whh(r*h_prev))\n",
        "        h_new = (1-z)*(h_prev) + z * g\n",
        "        return h_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecEq4TP2lZ4Z"
      },
      "source": [
        "## Step 2: GRU Encoder\n",
        "\n",
        "The following cells use your `MyGRUCell` implementation to build a recurrent encoder and decoder. Please read the implementations to understand what they do and run the cells before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jDNim2fmVJV"
      },
      "outputs": [],
      "source": [
        "class GRUEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(GRUEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = MyGRUCell(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "        annotations = []\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = encoded[:, i, :]  # Get the current time step, across the whole batch\n",
        "            hidden = self.gru(x, hidden)\n",
        "            annotations.append(hidden)\n",
        "\n",
        "        annotations = torch.stack(annotations, dim=1)\n",
        "        return annotations, hidden\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
        "        of a batch of sequences.\n",
        "\n",
        "        Arguments:\n",
        "            bs: The batch size for the initial hidden state.\n",
        "\n",
        "        Returns:\n",
        "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvwizYM9ma4p"
      },
      "outputs": [],
      "source": [
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
        "            annotations: This is not used here. It just maintains consistency with the\n",
        "                    interface used by the AttentionDecoder class.\n",
        "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            None\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = embed[\n",
        "                :, i, :\n",
        "            ]  # Get the current time step input tokens, across the whole batch\n",
        "            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSDTbsydlaGI"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "\n",
        "Train the encoder-decoder model to perform English --> Pig Latin translation. We will start by training on the smaller dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime"
      ],
      "metadata": {
        "id": "EzO0SyNjr2a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmVuXTozTPF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8caef24d-27a1-4cca-b135-d9ada405263e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 20                                     \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('overheard', 'overheardway')\n",
            "('accustomary', 'accustomaryway')\n",
            "('remembered', 'ememberedray')\n",
            "('vehemence', 'ehemencevay')\n",
            "('weddings', 'eddingsway')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.273 | Val loss: 1.996 | Gen: ingay ingay-ingay ontay ingay ingay\n",
            "Epoch:   1 | Train loss: 1.874 | Val loss: 1.833 | Gen: ay-ongay-onsay-onsay atingay oonsay-ongay ingay oonsay-ongay-onsay-o\n",
            "Epoch:   2 | Train loss: 1.704 | Val loss: 1.734 | Gen: eay-ay-ontay-ontay-a ay-ay-ingway otay-ongay-ontay-ont ingway oonsay-ontay-ontay-o\n",
            "Epoch:   3 | Train loss: 1.586 | Val loss: 1.664 | Gen: eay-ay-ay-ay-ay-ay-a ay-ay-ay-ay-ay-ay-ay otay-ay-ontay-ay-ay- iway oodgay-ingway\n",
            "Epoch:   4 | Train loss: 1.495 | Val loss: 1.620 | Gen: eway away ontentway iway oodgay\n",
            "Epoch:   5 | Train loss: 1.415 | Val loss: 1.566 | Gen: eationcay away onsingsay-ingway iway ortay-onday-ingway\n",
            "Epoch:   6 | Train loss: 1.342 | Val loss: 1.540 | Gen: eationcay away onsingsay-ingway iway otay-ingway\n",
            "Epoch:   7 | Train loss: 1.278 | Val loss: 1.511 | Gen: eay-ay-awlay away onsingsay-ingway iway ongay-ingway\n",
            "Epoch:   8 | Train loss: 1.230 | Val loss: 1.521 | Gen: eay-ay-awlay away onsingsay-inedway-in iway onday-inedway-ineway\n",
            "Epoch:   9 | Train loss: 1.186 | Val loss: 1.541 | Gen: eay-ay-awlay away onsingsay-inway-away iway ongray-ortioncay\n",
            "Epoch:  10 | Train loss: 1.136 | Val loss: 1.501 | Gen: eay-awlay away onsingway-ineway-inw iway ongray-ondedway\n",
            "Epoch:  11 | Train loss: 1.087 | Val loss: 1.479 | Gen: ehay away onsingsay-iecepay iway orkoussway\n",
            "Epoch:  12 | Train loss: 1.053 | Val loss: 1.486 | Gen: eay-ay-awlay arway oningsay-inway-awlay iway omourtnay\n",
            "Epoch:  13 | Train loss: 1.026 | Val loss: 1.486 | Gen: ehay awlay onsingway isway oktyway\n",
            "Epoch:  14 | Train loss: 1.013 | Val loss: 1.466 | Gen: ehay away onsingay-inway-awlay iway orkentway-iecepay\n",
            "Epoch:  15 | Train loss: 0.991 | Val loss: 1.430 | Gen: eay-ableway amay-away onsintingway isway oktray-omedway\n",
            "Epoch:  16 | Train loss: 0.957 | Val loss: 1.438 | Gen: eay-ableway arway onsingay-inway-andwa iway orkengsay-ortionway\n",
            "Epoch:  17 | Train loss: 0.937 | Val loss: 1.492 | Gen: ehay arway onsingay-ingsay-inwa isway orkentway-inway-awla\n",
            "Epoch:  18 | Train loss: 0.922 | Val loss: 1.403 | Gen: eay-abletay artway onsingway isway orkfaldedway\n",
            "Epoch:  19 | Train loss: 0.895 | Val loss: 1.428 | Gen: eay-abletay arway onsintingway-etetena isway omkonsay-awlay\n",
            "Epoch:  20 | Train loss: 0.876 | Val loss: 1.403 | Gen: eay-abletay aissway ondingway isway omkonsway\n",
            "Epoch:  21 | Train loss: 0.879 | Val loss: 1.513 | Gen: ehay-awlay aritway ondilightenedway isway omkondsay-andway-yba\n",
            "Epoch:  22 | Train loss: 0.887 | Val loss: 1.446 | Gen: ehay awlay ondingway-andway-and iway orkfay-ingsay-andway\n",
            "Epoch:  23 | Train loss: 0.858 | Val loss: 1.364 | Gen: ehay arway oningsay-abletay isway orkessway-iecepay\n",
            "Epoch:  24 | Train loss: 0.823 | Val loss: 1.381 | Gen: eay-abletay aritway oningualingday isway orkfay-andway-ybay\n",
            "Epoch:  25 | Train loss: 0.803 | Val loss: 1.362 | Gen: ehay arway ondilyway isway omkonway\n",
            "Epoch:  26 | Train loss: 0.802 | Val loss: 1.396 | Gen: ehay-awlay ariway onsidiceway iway orkfay-imedtay\n",
            "Epoch:  27 | Train loss: 0.807 | Val loss: 1.394 | Gen: ehay arway ondilyway isway orksway-awlay\n",
            "Epoch:  28 | Train loss: 0.790 | Val loss: 1.345 | Gen: ehay ariway oninouscay isway omkonsday\n",
            "Epoch:  29 | Train loss: 0.751 | Val loss: 1.372 | Gen: ehay awlay onsidiciessway isway orkisedway\n",
            "Epoch:  30 | Train loss: 0.743 | Val loss: 1.374 | Gen: ehay awray onsidiceway isway ommormicationcay\n",
            "Epoch:  31 | Train loss: 0.737 | Val loss: 1.488 | Gen: ehay aritationmay ondiliway-iecepay isway omkstay-awlay\n",
            "Epoch:  32 | Train loss: 0.744 | Val loss: 1.333 | Gen: ehay awray ondingsay-anterray isway omkonday\n",
            "Epoch:  33 | Train loss: 0.721 | Val loss: 1.340 | Gen: ehay aiseray ondingsay-abletay isway orkway-ightenesway\n",
            "Epoch:  34 | Train loss: 0.713 | Val loss: 1.375 | Gen: ehay ariway ondingway isway orkway-ightnay\n",
            "Epoch:  35 | Train loss: 0.720 | Val loss: 1.368 | Gen: ehay arway ondilicationmay isway omkantationday\n",
            "Epoch:  36 | Train loss: 0.704 | Val loss: 1.377 | Gen: ehay awray ondingway isway omkonsway-abletay\n",
            "Epoch:  37 | Train loss: 0.692 | Val loss: 1.371 | Gen: eheway aricay ondingday isway orkway-away-etetay\n",
            "Epoch:  38 | Train loss: 0.687 | Val loss: 1.394 | Gen: ehay aissway onsidionsday isway ormorfaltfay\n",
            "Epoch:  39 | Train loss: 0.687 | Val loss: 1.369 | Gen: ehay arway ondingway iway ormoncay-antasusway\n",
            "Epoch:  40 | Train loss: 0.669 | Val loss: 1.345 | Gen: ehay aricay onsidiceway isway ormorcay-iecepay\n",
            "Epoch:  41 | Train loss: 0.647 | Val loss: 1.338 | Gen: ehay arway ondingday-iecepay isway omksangingway\n",
            "Epoch:  42 | Train loss: 0.636 | Val loss: 1.353 | Gen: ehay aricay ondingday isway ormorcray-iecepay\n",
            "Epoch:  43 | Train loss: 0.632 | Val loss: 1.395 | Gen: ehay aricay ondingday isway urmorficationmay\n",
            "Epoch:  44 | Train loss: 0.647 | Val loss: 1.465 | Gen: ehay arway onitouctionmay iway arkway-awlay\n",
            "Epoch:  45 | Train loss: 0.668 | Val loss: 1.384 | Gen: ehay aricay ondincuationcay isway urkway-awlay\n",
            "Epoch:  46 | Train loss: 0.659 | Val loss: 1.398 | Gen: eheway aricay ondingday isway orkway-awlay\n",
            "Epoch:  47 | Train loss: 0.653 | Val loss: 1.386 | Gen: ehay aricay ondingway isway omkonsay\n",
            "Epoch:  48 | Train loss: 0.620 | Val loss: 1.371 | Gen: ehay aricay ondingday isway ormorchay\n",
            "Epoch:  49 | Train loss: 0.607 | Val loss: 1.362 | Gen: ehay aricay ondingway isway orkway-awlay\n",
            "Obtained lowest validation loss of: 1.333208414224478\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehay aricay ondingway isway orkway-awlay\n",
            "0:03:31.968673\n"
          ]
        }
      ],
      "source": [
        "original = datetime.datetime.now()\n",
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "rnn_args_s = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 50,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"early_stopping_patience\": 20,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 32,\n",
        "    \"encoder_type\": \"rnn\",  # options: rnn / transformer\n",
        "    \"decoder_type\": \"rnn\",  # options: rnn / rnn_attention / transformer\n",
        "    \"attention_type\": \"\",   # options: additive / scaled_dot\n",
        "}\n",
        "rnn_args_s.update(args_dict)\n",
        "\n",
        "print_opts(rnn_args_s)\n",
        "rnn_encode_s, rnn_decoder_s, rnn_losses_s = train(rnn_args_s)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, rnn_encode_s, rnn_decoder_s, None, rnn_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
        "print(datetime.datetime.now() - original)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mR97V_NtER6"
      },
      "source": [
        "Next, we train on the larger dataset. This experiment investigates if increasing dataset size improves model generalization on the validation set.\n",
        "\n",
        "For a fair comparison, the number of iterations (not number of epochs) for each run should be similar. This is done in a quick and dirty way by adjusting the batch size so approximately the same number of batches is processed per epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3YLrAjsmx_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "704d2a04-b185-46cb-d357-37e31700f603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 10                                     \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('bg', 'bgay')\n",
            "('grow', 'owgray')\n",
            "('intriguing', 'intriguingway')\n",
            "('cabernet', 'abernetcay')\n",
            "('wants', 'antsway')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.265 | Val loss: 2.146 | Gen: ontay-ay-ay-ay-ay-ay ay-ay-ay-ay-ay-ay-ay ontay-ay-ay-ay-ay-ay intay-ay-ay-ay-ay-ay ontay-ay-ay-ay-ay-ay\n",
            "Epoch:   1 | Train loss: 1.860 | Val loss: 2.017 | Gen: edway-ay-ay-ay-ay-ay ay-ay-ay-ay-ay-ay-ay ontay-ay-ay-ay-ay-ay inway-ay-ay-ay-ay-ay ontay-ay-ay-ay-ay-ay\n",
            "Epoch:   2 | Train loss: 1.709 | Val loss: 1.897 | Gen: edway away ontay-ontay-inway inway outhay-inway\n",
            "Epoch:   3 | Train loss: 1.587 | Val loss: 1.825 | Gen: edway away ontay-ingay-ayday inway outay-inway\n",
            "Epoch:   4 | Train loss: 1.482 | Val loss: 1.782 | Gen: elway-ingway-ayday away-away oungay-ingway ishedway oushingway\n",
            "Epoch:   5 | Train loss: 1.403 | Val loss: 1.710 | Gen: estay away-away ongay-ingray ishedway outingway\n",
            "Epoch:   6 | Train loss: 1.323 | Val loss: 1.689 | Gen: estay-ieway away-ieway ongingway ishedway ouplingway\n",
            "Epoch:   7 | Train loss: 1.267 | Val loss: 1.683 | Gen: etay away-ieway ongingay ishway oushedway\n",
            "Epoch:   8 | Train loss: 1.223 | Val loss: 1.692 | Gen: eway away ongingway ishway oughway\n",
            "Epoch:   9 | Train loss: 1.180 | Val loss: 1.693 | Gen: eway away ongay-ay ishway oukingway\n",
            "Epoch:  10 | Train loss: 1.142 | Val loss: 1.594 | Gen: etay imay-ieway ongingay-ay-ondway ishway orkingway\n",
            "Epoch:  11 | Train loss: 1.102 | Val loss: 1.664 | Gen: etay iaway-inway-awlay ongentay-ingray-awla isshay-inway orproushingway\n",
            "Epoch:  12 | Train loss: 1.076 | Val loss: 1.564 | Gen: etay away-inway ongingsay issway oukingway\n",
            "Epoch:  13 | Train loss: 1.012 | Val loss: 1.522 | Gen: etway iaway ongingsay issway oukingway\n",
            "Epoch:  14 | Train loss: 0.970 | Val loss: 1.530 | Gen: etway iarway ongingsay-inway ispay oukingway\n",
            "Epoch:  15 | Train loss: 0.942 | Val loss: 1.497 | Gen: etway aimay ongingsay-inway-awla ispay ousillway\n",
            "Epoch:  16 | Train loss: 0.927 | Val loss: 1.521 | Gen: etay aimay ongingshay-awlay isway orkingway\n",
            "Epoch:  17 | Train loss: 0.939 | Val loss: 1.562 | Gen: etway axayday oninghay-inway-awlay ispay oukinglay\n",
            "Epoch:  18 | Train loss: 0.949 | Val loss: 1.498 | Gen: etay iraway oningsay-ingray isway oukingtay\n",
            "Epoch:  19 | Train loss: 0.890 | Val loss: 1.457 | Gen: etay aiway ongingsay-ingray issway oukinglay\n",
            "Epoch:  20 | Train loss: 0.856 | Val loss: 1.456 | Gen: etay aiway ongingshay-inway-awl isway okingway\n",
            "Epoch:  21 | Train loss: 0.840 | Val loss: 1.503 | Gen: ethay aiway ongingshay-oushingwa isway orkinghay\n",
            "Epoch:  22 | Train loss: 0.820 | Val loss: 1.432 | Gen: etay airway ongingsay-ingronmay isway okingway\n",
            "Epoch:  23 | Train loss: 0.807 | Val loss: 1.504 | Gen: ethay irway ongingshay-inway isway okqueray\n",
            "Epoch:  24 | Train loss: 0.805 | Val loss: 1.477 | Gen: ethay iraway ongingshay-oushingwa issway okquray\n",
            "Epoch:  25 | Train loss: 0.786 | Val loss: 1.408 | Gen: ethay irawlay ongingshay-inway-awl isway okinghay\n",
            "Epoch:  26 | Train loss: 0.768 | Val loss: 1.425 | Gen: ethay axayday oningshay-ointmay isway okirgay\n",
            "Epoch:  27 | Train loss: 0.759 | Val loss: 1.492 | Gen: ethay airway ongingsay-inway isway okquay\n",
            "Epoch:  28 | Train loss: 0.765 | Val loss: 1.523 | Gen: ethay irway oninglay-antnay isway okqueray\n",
            "Epoch:  29 | Train loss: 0.791 | Val loss: 1.420 | Gen: ethay irway oningsay-ingronedway isway orkingway\n",
            "Epoch:  30 | Train loss: 0.747 | Val loss: 1.442 | Gen: ethay airway onsingsay-ingrowlay isway orkinghay\n",
            "Epoch:  31 | Train loss: 0.713 | Val loss: 1.372 | Gen: ethay airway oningshay-ouningway isway orkinghay\n",
            "Epoch:  32 | Train loss: 0.683 | Val loss: 1.383 | Gen: ethay iraway onsingsay-ingray isway orkingway\n",
            "Epoch:  33 | Train loss: 0.690 | Val loss: 1.406 | Gen: ethay airway onsinggray-isterstay isway orkingay\n",
            "Epoch:  34 | Train loss: 0.687 | Val loss: 1.358 | Gen: ethay airway onsingsuristingray isway orkinghay\n",
            "Epoch:  35 | Train loss: 0.657 | Val loss: 1.364 | Gen: ethay irway onsingighay-oushinal isway orkinghay\n",
            "Epoch:  36 | Train loss: 0.640 | Val loss: 1.337 | Gen: ethay airway onsinggay-ingray isway orkinghay\n",
            "Epoch:  37 | Train loss: 0.631 | Val loss: 1.339 | Gen: ethay irway onsingshay-ingray isway orkingway\n",
            "Epoch:  38 | Train loss: 0.640 | Val loss: 1.427 | Gen: ethay airway oniltinggay-inway-an isway orkinglay\n",
            "Epoch:  39 | Train loss: 0.636 | Val loss: 1.418 | Gen: ethay airway onspray-ingray-ondic isway orkinghay\n",
            "Epoch:  40 | Train loss: 0.634 | Val loss: 1.412 | Gen: ethay airway onsinglay-oushingway issway orkingway\n",
            "Epoch:  41 | Train loss: 0.632 | Val loss: 1.381 | Gen: ethay axayday onillsay-ingray-awla isway orkingay\n",
            "Epoch:  42 | Train loss: 0.641 | Val loss: 1.413 | Gen: ethay airway onsingingicutinallyw isway orkinghay\n",
            "Epoch:  43 | Train loss: 0.627 | Val loss: 1.387 | Gen: ethay airway oningisulitionspay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.643 | Val loss: 1.366 | Gen: ethay airway onsingdingsiagway isway orkingway\n",
            "Epoch:  45 | Train loss: 0.605 | Val loss: 1.323 | Gen: ethay airway onsingdicitionway isway orkingway\n",
            "Epoch:  46 | Train loss: 0.570 | Val loss: 1.296 | Gen: ethay airway onsingdictimay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.559 | Val loss: 1.340 | Gen: ethay airway onsingdicationsway isway orkingway\n",
            "Epoch:  48 | Train loss: 0.541 | Val loss: 1.271 | Gen: ethay airway onsingdicitidway isway orkingay\n",
            "Epoch:  49 | Train loss: 0.531 | Val loss: 1.338 | Gen: ethay airway onsingdicituriclay isway orkingway\n",
            "Obtained lowest validation loss of: 1.2714466374988358\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onsingdicituriclay isway orkingway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "rnn_args_l = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_large\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 50,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"early_stopping_patience\": 10,\n",
        "    \"batch_size\": 512,\n",
        "    \"hidden_size\": 32,\n",
        "    \"encoder_type\": \"rnn\",  # options: rnn / transformer\n",
        "    \"decoder_type\": \"rnn\",  # options: rnn / rnn_attention / transformer\n",
        "    \"attention_type\": \"\",   # options: additive / scaled_dot\n",
        "}\n",
        "rnn_args_l.update(args_dict)\n",
        "\n",
        "print_opts(rnn_args_l)\n",
        "rnn_encode_l, rnn_decoder_l, rnn_losses_l = train(rnn_args_l)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, rnn_encode_l, rnn_decoder_l, None, rnn_args_l\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01HsZ6EItc56"
      },
      "source": [
        "The code below plots the training and validation losses of each model, as a function of the number of gradient descent iterations. Are there significant differences in the validation performance of each model? (see follow-up questions in handout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qyk_9-Fwtekj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "df2caf0b-286e-43d7-9db5-cd752e5adc13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plot saved to: /content/content/csc421/a3/content/csc421/a3/content/csc421/a3/loss_plot_gru.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "save_loss_comparison_gru(rnn_losses_s, rnn_losses_l, rnn_args_s, rnn_args_l, \"gru\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE4ijaCzneAt"
      },
      "source": [
        "Select best performing model, and try translating different sentences by changing the variable TEST_SENTENCE. Identify a failure mode and briefly describe it (see follow-up questions in handout)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrNnz8W1nULf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "676078c5-4d0a-4342-b991-f37f1e575c53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onsingdicituriclay isway orkingway\n"
          ]
        }
      ],
      "source": [
        "best_encoder = rnn_encode_l  # Replace with rnn_encode_s or rnn_encode_l\n",
        "best_decoder = rnn_decoder_l  # Replace with rnn_decoder_s or rnn_decoder_l\n",
        "best_args = rnn_args_l     # Replace with rnn_args_s or rnn_args_l\n",
        "\n",
        "#TEST_SENTENCE = \"the air conditioning is working\"\n",
        "#TEST_SENTENCE = \"shot the short chipmunk with thick bullet\"\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, best_encoder, best_decoder, None, best_args\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Written Response\n",
        "\n",
        "1. When the model is trained on a larger dataset, it achieved better results. We can see this from the graph that at the final epoch, the model that was trained with a larger dataset achieved a lower validation loss than the model trained wiht small dataset, the lowest validation loss in the model trained with bigger dataset is also lower. This is becaues it learned more words and thus generalize and perform better.\n",
        "\n",
        "2. It seems that words that contains consanant pairs e.g: \"sh\", \"th\" , \"wh\" and \"ch\". Apart from the TEST_SENTENCE provided, I have also tried \"shot the short chipmunk with thick bullet\", which got translated into \"otssay etcay ortsway impostroupsway ithway icktay ulberay-eantway\". This might be caused by not having enough examples in the training data even in the large dataset.\n",
        "\n",
        "3. LSTM Encoder: **4K * (D*H + H^2)**\n",
        "GRU Encoder: **3K * (D* H + H^2)**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "izGZjHfKf6TM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWwA6OGqlaTq"
      },
      "source": [
        "# Part 2: Attention mechanisms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJSafHSAmu_w"
      },
      "source": [
        "## Step 1: Additive attention\n",
        "\n",
        "In the next cell, the [additive attention](https://paperswithcode.com/method/additive-attention) mechanism has been implemented for you. Please take a momement to read through it and understand what it is doing. See the assignment handouts for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdewEVSMo5jJ"
      },
      "outputs": [],
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # A two layer fully-connected network\n",
        "        # hidden_size * 2 --> hidden_size, ReLU, hidden_size --> 1\n",
        "        self.attention_network = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1),\n",
        "        )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the additive attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "        batch_size = keys.size(0)\n",
        "        expanded_queries = queries.view(batch_size, -1, self.hidden_size).expand_as(\n",
        "            keys\n",
        "        )\n",
        "        concat_inputs = torch.cat([expanded_queries, keys], dim=2)\n",
        "        unnormalized_attention = self.attention_network(concat_inputs)\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(attention_weights.transpose(2, 1), values)\n",
        "        return context, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73_p8d5EmvOJ"
      },
      "source": [
        "## Step 2: RNN + additive attention\n",
        "\n",
        "In the next cell, a modification of our `RNNDecoder` that makes use of an additive attention mechanism as been implemented for your. Please take a momement to read through it and understand what it is doing. See the assignment handouts for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJaABkXrpJSw"
      },
      "outputs": [],
      "source": [
        "class RNNAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attention_type=\"scaled_dot\"):\n",
        "        super(RNNAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size * 2, hidden_size=hidden_size)\n",
        "        if attention_type == \"additive\":\n",
        "            self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
        "        elif attention_type == \"scaled_dot\":\n",
        "            self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
        "\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        hiddens = []\n",
        "        attentions = []\n",
        "        h_prev = hidden_init\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            embed_current = embed[\n",
        "                :, i, :\n",
        "            ]  # Get the current time step, across the whole batch\n",
        "            context, attention_weights = self.attention(\n",
        "                h_prev, annotations, annotations\n",
        "            )  # batch_size x 1 x hidden_size\n",
        "            embed_and_context = torch.cat(\n",
        "                [embed_current, context.squeeze(1)], dim=1\n",
        "            )  # batch_size x (2*hidden_size)\n",
        "            h_prev = self.rnn(embed_and_context, h_prev)  # batch_size x hidden_size\n",
        "\n",
        "            hiddens.append(h_prev)\n",
        "            attentions.append(attention_weights)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1)  # batch_size x seq_len x hidden_size\n",
        "        attentions = torch.cat(attentions, dim=2)  # batch_size x seq_len x seq_len\n",
        "\n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, attentions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYPae08Io1Fi"
      },
      "source": [
        "## Step 3: Training and analysis (with additive attention)\n",
        "\n",
        "Now, run the following cell to train our recurrent encoder-decoder model with additive attention. How does it perform compared to the recurrent encoder-decoder model without attention?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime"
      ],
      "metadata": {
        "id": "kjaJWacjMh_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ke6t6rCezpZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abe29ac5-de86-4504-fbfd-6e2877acaa69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 10                                     \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn_attention                          \n",
            "                         attention_type: additive                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('overheard', 'overheardway')\n",
            "('accustomary', 'accustomaryway')\n",
            "('remembered', 'ememberedray')\n",
            "('vehemence', 'ehemencevay')\n",
            "('weddings', 'eddingsway')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 1.935 | Val loss: 1.711 | Gen: etttay arsay oonday issay oray\n",
            "Epoch:   1 | Train loss: 1.410 | Val loss: 1.537 | Gen: etay arday oootiontiontiontiont isway oray\n",
            "Epoch:   2 | Train loss: 1.135 | Val loss: 1.294 | Gen: ethay arway oncitindway isssay orgedway\n",
            "Epoch:   3 | Train loss: 0.961 | Val loss: 1.217 | Gen: eththay arway ondininindingincingi isway iorkingway\n",
            "Epoch:   4 | Train loss: 0.779 | Val loss: 1.052 | Gen: ethay away-irway ondentingingingingin isway orkoringinginginging\n",
            "Epoch:   5 | Train loss: 0.614 | Val loss: 0.935 | Gen: ethay arway ondinininininoningin issway orkingsay\n",
            "Epoch:   6 | Train loss: 0.477 | Val loss: 0.696 | Gen: ehay airway-iray onditiondingingingin isway orkingingway\n",
            "Epoch:   7 | Train loss: 0.329 | Val loss: 0.760 | Gen: ethay airway onditionway isway orkingway\n",
            "Epoch:   8 | Train loss: 0.322 | Val loss: 0.625 | Gen: ethay ayway onditioncay isway orkingway\n",
            "Epoch:   9 | Train loss: 0.230 | Val loss: 0.702 | Gen: ethay-uththay airway onditioncay isway orkingway\n",
            "Epoch:  10 | Train loss: 0.207 | Val loss: 0.671 | Gen: ethay airwayway ondititionway isway orkingway\n",
            "Epoch:  11 | Train loss: 0.169 | Val loss: 0.466 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  12 | Train loss: 0.149 | Val loss: 0.701 | Gen: ethay-othay airway onditionicingcay isway orkingway\n",
            "Epoch:  13 | Train loss: 0.161 | Val loss: 0.380 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  14 | Train loss: 0.103 | Val loss: 0.513 | Gen: ethay airway onditiondcay isway orkingway\n",
            "Epoch:  15 | Train loss: 0.083 | Val loss: 0.346 | Gen: ethay airway onditionway isway orkingway\n",
            "Epoch:  16 | Train loss: 0.063 | Val loss: 0.311 | Gen: ethay airway onditionway isway orkingway\n",
            "Epoch:  17 | Train loss: 0.048 | Val loss: 0.335 | Gen: ethay airway onditionway isway orkingway\n",
            "Epoch:  18 | Train loss: 0.048 | Val loss: 0.328 | Gen: ethay airway onditioningway isway orkingway\n",
            "Epoch:  19 | Train loss: 0.047 | Val loss: 0.335 | Gen: ethay airway onditioningway isway orkingway\n",
            "Epoch:  20 | Train loss: 0.036 | Val loss: 0.259 | Gen: ethay airway onditiondcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.023 | Val loss: 0.254 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.021 | Val loss: 0.233 | Gen: ethay airway onditionicingcay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.024 | Val loss: 0.231 | Gen: ethay airway onditionicingcay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.017 | Val loss: 0.222 | Gen: ethay airway onditionicingcay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.014 | Val loss: 0.264 | Gen: ethay airway onditionicingcay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.016 | Val loss: 0.209 | Gen: ethay airway onditionicingcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.035 | Val loss: 0.650 | Gen: ethay airway onotionicingcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.165 | Val loss: 0.838 | Gen: etthatty airway onditionicinininfica isway orkingway\n",
            "Epoch:  29 | Train loss: 0.340 | Val loss: 0.722 | Gen: ethay airway onditioncay isway orwingway\n",
            "Epoch:  30 | Train loss: 0.244 | Val loss: 0.671 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.154 | Val loss: 0.364 | Gen: ethay airway onditiontingcay isway orlingway\n",
            "Epoch:  32 | Train loss: 0.091 | Val loss: 0.377 | Gen: etay airway onditionicingcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.056 | Val loss: 0.290 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.035 | Val loss: 0.247 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.018 | Val loss: 0.250 | Gen: ethay airway onditionicingcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.010 | Val loss: 0.238 | Gen: ethay airway onditionicingcay isway orkingway\n",
            "Validation loss has not improved in 10 epochs, stopping early\n",
            "Obtained lowest validation loss of: 0.20935316750994667\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditionicingcay isway orkingway\n",
            "0:03:27.636694\n"
          ]
        }
      ],
      "source": [
        "original = datetime.datetime.now()\n",
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "rnn_attn_args = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 50,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"early_stopping_patience\": 10,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 64,\n",
        "    \"encoder_type\": \"rnn\",            # options: rnn / transformer\n",
        "    \"decoder_type\": \"rnn_attention\",  # options: rnn / rnn_attention / transformer\n",
        "    \"attention_type\": \"additive\",     # options: additive / scaled_dot\n",
        "}\n",
        "rnn_attn_args.update(args_dict)\n",
        "\n",
        "print_opts(rnn_attn_args)\n",
        "rnn_attn_encoder, rnn_attn_decoder, rnn_attn_losses = train(rnn_attn_args)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
        "print(datetime.datetime.now() - original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNVKbLc0ACj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebc06801-067c-4b4c-ed3d-3ebaabb0a5e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Written Response\n",
        "3) The training speed of the addition attention model is slightly longer (6seconds longer) than the original one (in above). The reason for that is although there is early stop, more time would be needed when more inputs were being fed at each step."
      ],
      "metadata": {
        "id": "zHyS8neApKuA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq7nhsEio1w-"
      },
      "source": [
        "## Step 4: Implement scaled dot-product attention\n",
        "\n",
        "In the next cell, you will implement the [scaled dot-product attention](https://paperswithcode.com/method/scaled) mechanism. See the assignment handouts for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_j3oY3hqsJQ"
      },
      "outputs": [],
      "source": [
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(\n",
        "            torch.tensor(self.hidden_size, dtype=torch.float)\n",
        "        )\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size = keys.shape[0]\n",
        "        q = self.Q(queries).view(batch_size, -1, self.hidden_size)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "        unnormalized_attention = torch.bmm(k, q.transpose(2, 1)) * self.scaling_factor\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(attention_weights.transpose(2, 1), v)\n",
        "        return context, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unReAOrjo113"
      },
      "source": [
        "## Step 5: Implement causal dot-product Attention\n",
        "\n",
        "\n",
        "Now, implement the casual scaled dot-product attention mechanism. It will be very similar to your implementation for `ScaledDotAttention`. The additional step is to mask out the attention to future timesteps so this attention mechanism can be used in a decoder. See the assignment handouts for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovigzQffrKqj"
      },
      "outputs": [],
      "source": [
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e7)\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(\n",
        "            torch.tensor(self.hidden_size, dtype=torch.float)\n",
        "        )\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size = keys.shape[0]\n",
        "        q = self.Q(queries).view(batch_size, -1, self.hidden_size)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "        unnormalized_attention = torch.bmm(k, q.transpose(2, 1)) * self.scaling_factor\n",
        "        mask = torch.tril(torch.ones(unnormalized_attention.shape), diagonal=-1).to(device ='cuda') * self.neg_inf\n",
        "        unnormalized_attention = unnormalized_attention + mask\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(attention_weights.transpose(2, 1), v)\n",
        "        return context, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkjHbtvT6Qxs"
      },
      "source": [
        "## Step 6: Attention encoder and decoder\n",
        "\n",
        "The following cells provide an implementation of an encoder and decoder that use a single `ScaledDotAttention` block. Please read through them to understand what they are doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKGNqUaX6RLO"
      },
      "outputs": [],
      "source": [
        "class AttentionEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(AttentionEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.self_attention = ScaledDotAttention(\n",
        "                    hidden_size=hidden_size,\n",
        "                )\n",
        "\n",
        "        self.attention_mlp = nn.Sequential(\n",
        "                                nn.Linear(hidden_size, hidden_size),\n",
        "                                nn.ReLU(),\n",
        "                              )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder scaled dot attention.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            None: Used to conform to standard encoder return signature.\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = inputs.size()\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        annotations = encoded\n",
        "        new_annotations, self_attention_weights = self.self_attention(\n",
        "            annotations, annotations, annotations\n",
        "        )  # batch_size x seq_len x hidden_size\n",
        "        residual_annotations = annotations + new_annotations\n",
        "        new_annotations = self.attention_mlp(residual_annotations)\n",
        "        annotations = residual_annotations + new_annotations\n",
        "\n",
        "        return annotations, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDUvtOee7cMy"
      },
      "outputs": [],
      "source": [
        "class AttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(AttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.self_attention = CausalScaledDotAttention(\n",
        "                                hidden_size=hidden_size,\n",
        "                                )\n",
        "\n",
        "        self.decoder_attention = ScaledDotAttention(\n",
        "                                  hidden_size=hidden_size,\n",
        "                                  )\n",
        "\n",
        "        self.attention_mlp = nn.Sequential(\n",
        "                                nn.Linear(hidden_size, hidden_size),\n",
        "                                nn.ReLU(),\n",
        "                              )\n",
        "\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        new_contexts, self_attention_weights = self.self_attention(\n",
        "            contexts, contexts, contexts\n",
        "        )  # batch_size x seq_len x hidden_size\n",
        "        residual_contexts = contexts + new_contexts\n",
        "        new_contexts, encoder_attention_weights = self.decoder_attention(\n",
        "            residual_contexts, annotations, annotations\n",
        "        )  # batch_size x seq_len x hidden_size\n",
        "        residual_contexts = residual_contexts + new_contexts\n",
        "        new_contexts = self.attention_mlp(residual_contexts)\n",
        "        contexts = residual_contexts + new_contexts\n",
        "\n",
        "        encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "        self_attention_weights_list.append(self_attention_weights)\n",
        "\n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "\n",
        "        return output, (encoder_attention_weights, self_attention_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7gJLw5t_rnW"
      },
      "source": [
        "## Step 7: Training and analysis (single scaled dot-product attention block)\n",
        "\n",
        "Now, train the following model, with an encoder and decoder each composed a single `ScaledDotAttention` block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MOkZonC8T3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6787b23c-3ccd-4736-87a4-3fb9f40720a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 100                                    \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: attention                              \n",
            "                           decoder_type: attention                              \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('overheard', 'overheardway')\n",
            "('accustomary', 'accustomaryway')\n",
            "('remembered', 'ememberedray')\n",
            "('vehemence', 'ehemencevay')\n",
            "('weddings', 'eddingsway')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.892 | Val loss: 2.448 | Gen: ay ay ay oesay ay   \n",
            "Epoch:   1 | Train loss: 2.255 | Val loss: 2.141 | Gen: eay ay onay oesay onay\n",
            "Epoch:   2 | Train loss: 2.042 | Val loss: 2.003 | Gen: eay ay ongngncay issssssssssssssssswa ongtingtingngtingngt\n",
            "Epoch:   3 | Train loss: 1.925 | Val loss: 1.914 | Gen: eteay isay ongay issssssssssssssssswa ongtingtingtingtingt\n",
            "Epoch:   4 | Train loss: 1.842 | Val loss: 1.847 | Gen: etetay isay ongay isssssssssssssssway ongay\n",
            "Epoch:   5 | Train loss: 1.776 | Val loss: 1.796 | Gen: etetay isay ongay issssssssssssway ongay\n",
            "Epoch:   6 | Train loss: 1.720 | Val loss: 1.754 | Gen: etethay isay ingingingay isssssssssway ongay\n",
            "Epoch:   7 | Train loss: 1.670 | Val loss: 1.719 | Gen: ethay iway ingingingingway isssssssway ongay\n",
            "Epoch:   8 | Train loss: 1.624 | Val loss: 1.688 | Gen: ethay iway ingingingway issssssway ongay\n",
            "Epoch:   9 | Train loss: 1.583 | Val loss: 1.662 | Gen: ethay iway ingingingway isssssway ongway\n",
            "Epoch:  10 | Train loss: 1.546 | Val loss: 1.638 | Gen: ethay iway ingingway issssway ongway\n",
            "Epoch:  11 | Train loss: 1.512 | Val loss: 1.613 | Gen: ethay iway ingingdingdingdway issssway ongway\n",
            "Epoch:  12 | Train loss: 1.479 | Val loss: 1.589 | Gen: ethay ay ingingdingdway issssway ongway\n",
            "Epoch:  13 | Train loss: 1.447 | Val loss: 1.567 | Gen: ethay ay ingdingdingdway issssway oway\n",
            "Epoch:  14 | Train loss: 1.418 | Val loss: 1.546 | Gen: ethay ay ingdingdingdway isssway oway\n",
            "Epoch:  15 | Train loss: 1.391 | Val loss: 1.527 | Gen: ethay ay ingdingdingdway isssway okingway\n",
            "Epoch:  16 | Train loss: 1.366 | Val loss: 1.510 | Gen: ethay ay ingdingdway isssway okingway\n",
            "Epoch:  17 | Train loss: 1.342 | Val loss: 1.495 | Gen: ethay ay ongingdway isssway okingway\n",
            "Epoch:  18 | Train loss: 1.320 | Val loss: 1.480 | Gen: ethay ay ingdingdway isssway okingway\n",
            "Epoch:  19 | Train loss: 1.299 | Val loss: 1.467 | Gen: ethay ay ongingdway issway okingway\n",
            "Epoch:  20 | Train loss: 1.279 | Val loss: 1.456 | Gen: ethay ay ongingdway issway okingway\n",
            "Epoch:  21 | Train loss: 1.260 | Val loss: 1.447 | Gen: ethay away ongingdway issway okingway\n",
            "Epoch:  22 | Train loss: 1.243 | Val loss: 1.438 | Gen: ethay away ongingdinay issway okingway\n",
            "Epoch:  23 | Train loss: 1.228 | Val loss: 1.430 | Gen: ethay away onginay issway okingway\n",
            "Epoch:  24 | Train loss: 1.213 | Val loss: 1.423 | Gen: ethay away onginay issway okingway\n",
            "Epoch:  25 | Train loss: 1.199 | Val loss: 1.416 | Gen: ethay away oningdingdingdinay issway okingway\n",
            "Epoch:  26 | Train loss: 1.186 | Val loss: 1.410 | Gen: ethay ay oningdingdinay issway okingway\n",
            "Epoch:  27 | Train loss: 1.173 | Val loss: 1.404 | Gen: ethay ay oningdingdinay isway okingway\n",
            "Epoch:  28 | Train loss: 1.161 | Val loss: 1.398 | Gen: ethay airay oningdingdinay isway okingway\n",
            "Epoch:  29 | Train loss: 1.150 | Val loss: 1.391 | Gen: ethay airay oningdingdinway isway okingway\n",
            "Epoch:  30 | Train loss: 1.139 | Val loss: 1.386 | Gen: ethay airay oningdincingdincingd isway okingway\n",
            "Epoch:  31 | Train loss: 1.128 | Val loss: 1.380 | Gen: ethay airay oningdincingdincingd isway okingway\n",
            "Epoch:  32 | Train loss: 1.118 | Val loss: 1.375 | Gen: ethay airay oningdincingdincingd isway okingway\n",
            "Epoch:  33 | Train loss: 1.108 | Val loss: 1.370 | Gen: ethay airay oningdincingdincingd isway okingway\n",
            "Epoch:  34 | Train loss: 1.099 | Val loss: 1.366 | Gen: ethay airay oningdincingdincingd isway okingway\n",
            "Epoch:  35 | Train loss: 1.090 | Val loss: 1.362 | Gen: ethay airay oningdincingdincinci isway okingway\n",
            "Epoch:  36 | Train loss: 1.082 | Val loss: 1.359 | Gen: ethay airay oningdincingdincinci isway okingway\n",
            "Epoch:  37 | Train loss: 1.074 | Val loss: 1.353 | Gen: ethay aiway oningdincingdincinci isway okingway\n",
            "Epoch:  38 | Train loss: 1.065 | Val loss: 1.349 | Gen: ethay aiway oningdincingdincinci isway okingway\n",
            "Epoch:  39 | Train loss: 1.058 | Val loss: 1.345 | Gen: ethay aiway oningdincingdincinci isway okingway\n",
            "Epoch:  40 | Train loss: 1.050 | Val loss: 1.341 | Gen: ethay aiway oningdincincingdinci isway okingway\n",
            "Epoch:  41 | Train loss: 1.043 | Val loss: 1.337 | Gen: ethay aiway oningdincincingdinci isway okingway\n",
            "Epoch:  42 | Train loss: 1.035 | Val loss: 1.334 | Gen: ethay aiway oningdincincingdinci isway okingway\n",
            "Epoch:  43 | Train loss: 1.029 | Val loss: 1.329 | Gen: ethay aiway oningdincincingdinci isway okingway\n",
            "Epoch:  44 | Train loss: 1.022 | Val loss: 1.325 | Gen: ethay aiway oningdincincingdinci isway okingway\n",
            "Epoch:  45 | Train loss: 1.015 | Val loss: 1.320 | Gen: ethay aiway oningdincincingdinci isway okingway\n",
            "Epoch:  46 | Train loss: 1.009 | Val loss: 1.317 | Gen: ethay aiway oningdincincingdinay isway okingway\n",
            "Epoch:  47 | Train loss: 1.003 | Val loss: 1.313 | Gen: ethay aiway oningdincincingdinci isway okingway\n",
            "Epoch:  48 | Train loss: 0.996 | Val loss: 1.310 | Gen: ethay aiway oningdincincingdinci isway okingway\n",
            "Epoch:  49 | Train loss: 0.991 | Val loss: 1.305 | Gen: ethay aiway oningdincincingdinci isway okingway\n",
            "Epoch:  50 | Train loss: 0.985 | Val loss: 1.303 | Gen: ethay aiway oningdincincingdinci isway okingway\n",
            "Epoch:  51 | Train loss: 0.979 | Val loss: 1.298 | Gen: ethay aiway oningdincincingdinci isway okingway\n",
            "Epoch:  52 | Train loss: 0.973 | Val loss: 1.296 | Gen: ethay aiway oningdincincingdinci isway okingway\n",
            "Epoch:  53 | Train loss: 0.968 | Val loss: 1.292 | Gen: ethay aiway oningdincincingdinci isway okingway\n",
            "Epoch:  54 | Train loss: 0.963 | Val loss: 1.288 | Gen: ethay aiway oningdincincingdinci isway okingway\n",
            "Epoch:  55 | Train loss: 0.958 | Val loss: 1.285 | Gen: ethay aiway oningdincincingdinci isway okingway\n",
            "Epoch:  56 | Train loss: 0.953 | Val loss: 1.282 | Gen: ethay aiway oningdincincindingdi isway okingway\n",
            "Epoch:  57 | Train loss: 0.948 | Val loss: 1.279 | Gen: ethay aiway oningdincincintindin isway okingway\n",
            "Epoch:  58 | Train loss: 0.943 | Val loss: 1.276 | Gen: ethay aiway onindingdincintindin isway okingway\n",
            "Epoch:  59 | Train loss: 0.938 | Val loss: 1.274 | Gen: ethay aiway onindingdincintindin isway okingway\n",
            "Epoch:  60 | Train loss: 0.934 | Val loss: 1.270 | Gen: ethay aiway onindingdincintindin isway okingway\n",
            "Epoch:  61 | Train loss: 0.929 | Val loss: 1.268 | Gen: ethay aiway onindingdincintindin isway okingway\n",
            "Epoch:  62 | Train loss: 0.925 | Val loss: 1.264 | Gen: ethay aiway onindingdincintindin isway okingway\n",
            "Epoch:  63 | Train loss: 0.920 | Val loss: 1.263 | Gen: ethay aiway onindingdincintindin isway okingway\n",
            "Epoch:  64 | Train loss: 0.916 | Val loss: 1.258 | Gen: ethay aiway onindingdincintindin isway okingway\n",
            "Epoch:  65 | Train loss: 0.912 | Val loss: 1.257 | Gen: ethay aiway onindingdincintindin isway okingway\n",
            "Epoch:  66 | Train loss: 0.908 | Val loss: 1.254 | Gen: ethay aiway onindingdincintindin isway okingway\n",
            "Epoch:  67 | Train loss: 0.904 | Val loss: 1.251 | Gen: ethay aiway onindingway isway okingway\n",
            "Epoch:  68 | Train loss: 0.900 | Val loss: 1.248 | Gen: ethay aiway onindingway isway okingway\n",
            "Epoch:  69 | Train loss: 0.897 | Val loss: 1.246 | Gen: ethay aiway onindingway isway okingway\n",
            "Epoch:  70 | Train loss: 0.893 | Val loss: 1.244 | Gen: ethay aiway onindingway isway okingway\n",
            "Epoch:  71 | Train loss: 0.889 | Val loss: 1.241 | Gen: ethay aiway onindingcintindindin isway okingway\n",
            "Epoch:  72 | Train loss: 0.886 | Val loss: 1.239 | Gen: ethay aiway onindingcintindindin isway okingway\n",
            "Epoch:  73 | Train loss: 0.882 | Val loss: 1.237 | Gen: ethay aiway onindingcintindindin isway okingway\n",
            "Epoch:  74 | Train loss: 0.879 | Val loss: 1.235 | Gen: ethay aiway onindingcintindindin isway okingway\n",
            "Epoch:  75 | Train loss: 0.875 | Val loss: 1.233 | Gen: ethay aiway onindingcintindindin isway okingway\n",
            "Epoch:  76 | Train loss: 0.872 | Val loss: 1.231 | Gen: ethay aiway onindingcintindindin isway okingway\n",
            "Epoch:  77 | Train loss: 0.869 | Val loss: 1.229 | Gen: ethay airiway onindingcintindindin isway okingway\n",
            "Epoch:  78 | Train loss: 0.866 | Val loss: 1.227 | Gen: ethay airiway onindingcintindindin isway okingway\n",
            "Epoch:  79 | Train loss: 0.863 | Val loss: 1.225 | Gen: ethay airiway onindingcintindindin isway okingway\n",
            "Epoch:  80 | Train loss: 0.860 | Val loss: 1.223 | Gen: ethay airiway onindingcintindindin isway okingway\n",
            "Epoch:  81 | Train loss: 0.857 | Val loss: 1.221 | Gen: ethay airiway onindingcintindindin isway okingway\n",
            "Epoch:  82 | Train loss: 0.855 | Val loss: 1.220 | Gen: ethay airiway onindingcintindindin isway okingway\n",
            "Epoch:  83 | Train loss: 0.852 | Val loss: 1.218 | Gen: ethay airiway onindingcintindindin isway okingway\n",
            "Epoch:  84 | Train loss: 0.849 | Val loss: 1.217 | Gen: ethay airiway onindingcintindindin isway okingway\n",
            "Epoch:  85 | Train loss: 0.847 | Val loss: 1.214 | Gen: ethay airiway onindingcintindindin isway okingway\n",
            "Epoch:  86 | Train loss: 0.844 | Val loss: 1.213 | Gen: ethay airiway onindingcintindindin isway okingway\n",
            "Epoch:  87 | Train loss: 0.841 | Val loss: 1.211 | Gen: ethay airiway onindingcintindindin isway okingway\n",
            "Epoch:  88 | Train loss: 0.839 | Val loss: 1.209 | Gen: ethay airiway onindingcintindindin isway okingway\n",
            "Epoch:  89 | Train loss: 0.837 | Val loss: 1.207 | Gen: ethay airiway onindingcintindindin isway okingway\n",
            "Epoch:  90 | Train loss: 0.834 | Val loss: 1.206 | Gen: ethay airiway onindingcintindindin isway okingway\n",
            "Epoch:  91 | Train loss: 0.832 | Val loss: 1.204 | Gen: ethay airiway onindingcintindindin isway okingway\n",
            "Epoch:  92 | Train loss: 0.830 | Val loss: 1.203 | Gen: ethay airiway onindingcintindindin isway okingway\n",
            "Epoch:  93 | Train loss: 0.827 | Val loss: 1.202 | Gen: ethay airiway onindingcintindindin isway okingway\n",
            "Epoch:  94 | Train loss: 0.825 | Val loss: 1.200 | Gen: ethay airiway onindingcintindindin isway okingway\n",
            "Epoch:  95 | Train loss: 0.823 | Val loss: 1.199 | Gen: ethay airiway onindingcintindindin isway okingway\n",
            "Epoch:  96 | Train loss: 0.821 | Val loss: 1.197 | Gen: exthay airiway onindingcintindindin isway okingway\n",
            "Epoch:  97 | Train loss: 0.819 | Val loss: 1.196 | Gen: exthehay airiway onindingcintindindin isway okingway\n",
            "Epoch:  98 | Train loss: 0.817 | Val loss: 1.195 | Gen: exthehay airiway onindingcintindindin isway oway\n",
            "Epoch:  99 | Train loss: 0.815 | Val loss: 1.193 | Gen: exthehay airiway onindingcintindindin isway oway\n",
            "Obtained lowest validation loss of: 1.1933680046827366\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\texthehay airiway onindingcintindindin isway oway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "attention_args_s = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 100,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 100,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 32,\n",
        "    \"encoder_type\": \"attention\",\n",
        "    \"decoder_type\": \"attention\",  # options: rnn / rnn_attention / attention / transformer\n",
        "}\n",
        "attention_args_s.update(args_dict)\n",
        "print_opts(attention_args_s)\n",
        "\n",
        "attention_encoder_s, attention_decoder_s, attention_losses_s = train(attention_args_s)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, attention_encoder_s, attention_decoder_s, None, attention_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Written Response:\n",
        "3. The single block attention model performs worse than the RNNAttention model. We can see that from the much bigger lowest validation loss here (RNNAttention: 0.2, single block model: 1.2). One reason for that is because more time is needed to process the keys, queries, and values pairs, implying an increased in the number of parameters, and hence need training steps would increase.\n",
        "\n",
        "4. Same word in different postions can have different meanings. The positional embeddings are useful because it indicates the order of the tokens while the model that we are training does not keep track of the orders. Using the sin and cos for position embedding is better than one hot encoding because sin(x+k) and cos(x+k) where k is the offset, can be expressed as a liner combination of sin and cos. Moreover, sin and cos would be within the range of [-1, 1], meaning that these embeddings will be continuous and can thus gives us more combinations with the same embedding dimension compare to one hot."
      ],
      "metadata": {
        "id": "9vSb5BUp6na9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tcpUFKqo2Oi"
      },
      "source": [
        "## Step 8: Transformer encoder and decoder\n",
        "\n",
        "The following cells provide an implementation of the transformer encoder and decoder that use your `ScaledDotAttention` and `CausalScaledDotAttention`. Please read through them to understand what they are doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3B-fWsarlVk"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers, opts):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.self_attentions = nn.ModuleList(\n",
        "            [\n",
        "                ScaledDotAttention(\n",
        "                    hidden_size=hidden_size,\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.attention_mlps = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(hidden_size, hidden_size),\n",
        "                    nn.ReLU(),\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            None: Used to conform to standard encoder return signature.\n",
        "            None: Used to conform to standard encoder return signature.\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = inputs.size()\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        # Add positinal embeddings from self.create_positional_encodings. (a'la https://arxiv.org/pdf/1706.03762.pdf, section 3.5)\n",
        "        encoded = encoded + self.positional_encodings[:seq_len]\n",
        "\n",
        "        annotations = encoded\n",
        "        for i in range(self.num_layers):\n",
        "            new_annotations, self_attention_weights = self.self_attentions[i](\n",
        "                annotations, annotations, annotations\n",
        "            )  # batch_size x seq_len x hidden_size\n",
        "            residual_annotations = annotations + new_annotations\n",
        "            new_annotations = self.attention_mlps[i](residual_annotations)\n",
        "            annotations = residual_annotations + new_annotations\n",
        "\n",
        "        # Transformer encoder does not have a last hidden or cell layer.\n",
        "        return annotations, None\n",
        "        # return annotations, None, None\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len.\n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "        dim_indices = torch.arange(self.hidden_size // 2)[None, ...]\n",
        "        exponents = (2 * dim_indices).float() / (self.hidden_size)\n",
        "        trig_args = pos_indices / (10000**exponents)\n",
        "        sin_terms = torch.sin(trig_args)\n",
        "        cos_terms = torch.cos(trig_args)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms\n",
        "        pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "        if self.opts.cuda:\n",
        "            pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyvTZFxtrvc6"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.self_attentions = nn.ModuleList(\n",
        "            [\n",
        "                CausalScaledDotAttention(\n",
        "                    hidden_size=hidden_size,\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.encoder_attentions = nn.ModuleList(\n",
        "            [\n",
        "                ScaledDotAttention(\n",
        "                    hidden_size=hidden_size,\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.attention_mlps = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(hidden_size, hidden_size),\n",
        "                    nn.ReLU(),\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        embed = embed + self.positional_encodings[:seq_len]\n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        for i in range(self.num_layers):\n",
        "            new_contexts, self_attention_weights = self.self_attentions[i](\n",
        "                contexts, contexts, contexts\n",
        "            )  # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = contexts + new_contexts\n",
        "            new_contexts, encoder_attention_weights = self.encoder_attentions[i](\n",
        "                residual_contexts, annotations, annotations\n",
        "            )  # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = residual_contexts + new_contexts\n",
        "            new_contexts = self.attention_mlps[i](residual_contexts)\n",
        "            contexts = residual_contexts + new_contexts\n",
        "\n",
        "            encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "            self_attention_weights_list.append(self_attention_weights)\n",
        "\n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "\n",
        "        return output, (encoder_attention_weights, self_attention_weights)\n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len.\n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "        dim_indices = torch.arange(self.hidden_size // 2)[None, ...]\n",
        "        exponents = (2 * dim_indices).float() / (self.hidden_size)\n",
        "        trig_args = pos_indices / (10000**exponents)\n",
        "        sin_terms = torch.sin(trig_args)\n",
        "        cos_terms = torch.cos(trig_args)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms\n",
        "        pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "        pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ZjkXTNrUKb"
      },
      "source": [
        "\n",
        "## Step 9: Training and analysis (with scaled dot-product attention)\n",
        "\n",
        "Now we will train a (simplified) transformer encoder-decoder model.\n",
        "\n",
        "First, we train our smaller model on the small dataset. Use this model to answer Question 5 in the handout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk8e4KSnuZ8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed305931-54aa-4e8d-e953-334e21d59773"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 100                                    \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 4                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('gales', 'alesgay')\n",
            "('palanquins', 'alanquinspay')\n",
            "('measuring', 'easuringmay')\n",
            "('weather', 'eatherway')\n",
            "('closing', 'osingclay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 3.250 | Val loss: 2.424 | Gen: eayfeayfay ay iy-y-ay-ay y ay-ay-ay-ay\n",
            "Epoch:   1 | Train loss: 2.109 | Val loss: 2.087 | Gen: eay ay indiay-ay-ay istisay ohinty-ay\n",
            "Epoch:   2 | Train loss: 1.833 | Val loss: 1.903 | Gen: etay ay ointinntinnnontay istisay ohinty\n",
            "Epoch:   3 | Train loss: 1.641 | Val loss: 1.775 | Gen: etay ay oinnnngay isay oingay\n",
            "Epoch:   4 | Train loss: 1.502 | Val loss: 1.635 | Gen: ehay ay oinnnngay-inay isay oringay\n",
            "Epoch:   5 | Train loss: 1.378 | Val loss: 1.587 | Gen: ehety arway oincay-oindinay isay ortingay\n",
            "Epoch:   6 | Train loss: 1.278 | Val loss: 1.559 | Gen: ehay iwaywayway oincay-ointintintay iway orrray\n",
            "Epoch:   7 | Train loss: 1.211 | Val loss: 1.469 | Gen: ehay iwaywayway ondingay-ongnay istay orray-oagray\n",
            "Epoch:   8 | Train loss: 1.139 | Val loss: 1.433 | Gen: ehay arway ondigay-ingnay isay orray-agngway\n",
            "Epoch:   9 | Train loss: 1.090 | Val loss: 1.420 | Gen: ehay arway onongay-onday isay orray-ongggway\n",
            "Epoch:  10 | Train loss: 1.023 | Val loss: 1.396 | Gen: ehthhay arway ondigay isway orray-agngway\n",
            "Epoch:  11 | Train loss: 1.008 | Val loss: 1.411 | Gen: ehshay arrway ondgay-ondingngday isway oongray\n",
            "Epoch:  12 | Train loss: 0.974 | Val loss: 1.341 | Gen: ehay arway ondigay isway oray-agngagway\n",
            "Epoch:  13 | Train loss: 0.896 | Val loss: 1.357 | Gen: ehay arrway ondigay-onay isway ordgay-agngay\n",
            "Epoch:  14 | Train loss: 0.839 | Val loss: 1.357 | Gen: ehay arrway ondigay-ongday isway orray-agngay\n",
            "Epoch:  15 | Train loss: 0.802 | Val loss: 1.288 | Gen: ehay arrway ondigay-odinngday isway orray-agngray\n",
            "Epoch:  16 | Train loss: 0.748 | Val loss: 1.345 | Gen: ehay arrway ondigay-ongdiay isway oray-agndgway\n",
            "Epoch:  17 | Train loss: 0.755 | Val loss: 1.410 | Gen: ehay arrway ongatindinay isway orray-y\n",
            "Epoch:  18 | Train loss: 0.750 | Val loss: 1.328 | Gen: ehay arrway ondingay-onindingday isway oray-ingngngay\n",
            "Epoch:  19 | Train loss: 0.700 | Val loss: 1.285 | Gen: ehay arrway ondingiay isway orkray\n",
            "Epoch:  20 | Train loss: 0.671 | Val loss: 1.313 | Gen: ehay iray ondingdingday isway orray-orngay\n",
            "Epoch:  21 | Train loss: 0.647 | Val loss: 1.264 | Gen: ehay irway ondigiayingiingday isway orrway-ingway\n",
            "Epoch:  22 | Train loss: 0.648 | Val loss: 1.191 | Gen: ehay irarway onditingdlay isway orkingay\n",
            "Epoch:  23 | Train loss: 0.633 | Val loss: 1.186 | Gen: ehay iray ondiningdininontindi isway orkringay\n",
            "Epoch:  24 | Train loss: 0.572 | Val loss: 1.123 | Gen: ehay irarway onditingdingway isway orkingay\n",
            "Epoch:  25 | Train loss: 0.528 | Val loss: 1.248 | Gen: ehay irway onditingdingny isway orkringay\n",
            "Epoch:  26 | Train loss: 0.510 | Val loss: 1.210 | Gen: ehay irway onditingdininingway isway orkingway\n",
            "Epoch:  27 | Train loss: 0.485 | Val loss: 1.179 | Gen: ehay irway onditingdingny isway orkingway\n",
            "Epoch:  28 | Train loss: 0.472 | Val loss: 1.177 | Gen: ehay arrway onditingdiny isway orkingray\n",
            "Epoch:  29 | Train loss: 0.455 | Val loss: 1.129 | Gen: ehay irway onditingdingway isway orkingway\n",
            "Epoch:  30 | Train loss: 0.434 | Val loss: 1.112 | Gen: ehay arrway onditingdingway isway orkingway\n",
            "Epoch:  31 | Train loss: 0.426 | Val loss: 1.229 | Gen: ehay irway onditingdingngdingwa isway orkingway\n",
            "Epoch:  32 | Train loss: 0.422 | Val loss: 1.122 | Gen: ehay arirway onditingingngway isway orwingdway\n",
            "Epoch:  33 | Train loss: 0.412 | Val loss: 1.232 | Gen: ehay irway onditingingngway isway orringdway\n",
            "Epoch:  34 | Train loss: 0.423 | Val loss: 1.178 | Gen: ehay iwarway ondigingdingway isway orway\n",
            "Epoch:  35 | Train loss: 0.452 | Val loss: 1.239 | Gen: ethay arway ndceiiiininiway isway oringway\n",
            "Epoch:  36 | Train loss: 0.472 | Val loss: 1.261 | Gen: ehay-thhtwwwway arrway onditiningdiay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.457 | Val loss: 1.004 | Gen: ehay irway ondcoiitingday isway orkingway\n",
            "Epoch:  38 | Train loss: 0.400 | Val loss: 0.971 | Gen: ehay ariway onditingintiay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.386 | Val loss: 0.982 | Gen: ehay ariway onditingingway isway orkingway\n",
            "Epoch:  40 | Train loss: 0.378 | Val loss: 0.961 | Gen: ehay ariway onditingdiay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.351 | Val loss: 1.014 | Gen: ethay ariway onditinginway isway orkingway\n",
            "Epoch:  42 | Train loss: 0.327 | Val loss: 0.987 | Gen: ehay ariway onditingway isway orkingway\n",
            "Epoch:  43 | Train loss: 0.303 | Val loss: 0.984 | Gen: ehay arway onditingway isway orkingway\n",
            "Epoch:  44 | Train loss: 0.289 | Val loss: 0.972 | Gen: ehay arway onditinginway isway orkingway\n",
            "Epoch:  45 | Train loss: 0.276 | Val loss: 0.959 | Gen: ehay arway onditinginway isway orkingway\n",
            "Epoch:  46 | Train loss: 0.266 | Val loss: 0.978 | Gen: ehay arway onditinginway isway orkingway\n",
            "Epoch:  47 | Train loss: 0.256 | Val loss: 0.948 | Gen: ehay arway onditinginway isway orkingway\n",
            "Epoch:  48 | Train loss: 0.246 | Val loss: 0.957 | Gen: ehay arway onditinginway isway orkingway\n",
            "Epoch:  49 | Train loss: 0.237 | Val loss: 0.964 | Gen: ehay arway onditinginway isway orkingway\n",
            "Epoch:  50 | Train loss: 0.230 | Val loss: 0.939 | Gen: ehay arway onditinginway isway orkingway\n",
            "Epoch:  51 | Train loss: 0.219 | Val loss: 0.948 | Gen: ehay arway onditinginway isway orkingway\n",
            "Epoch:  52 | Train loss: 0.212 | Val loss: 0.926 | Gen: ethay arway ondciiitinway isway orkingway\n",
            "Epoch:  53 | Train loss: 0.201 | Val loss: 0.900 | Gen: ehay arway onditincay isway orkingway\n",
            "Epoch:  54 | Train loss: 0.191 | Val loss: 0.893 | Gen: ethay arway onditinginway isway orkingway\n",
            "Epoch:  55 | Train loss: 0.182 | Val loss: 0.895 | Gen: ethay arrwaway ondciiitiniway isway orkingway\n",
            "Epoch:  56 | Train loss: 0.177 | Val loss: 0.888 | Gen: ethay arway onditingintiay isway orkingway\n",
            "Epoch:  57 | Train loss: 0.169 | Val loss: 0.887 | Gen: ethay arway onditintingway isway orkingay\n",
            "Epoch:  58 | Train loss: 0.162 | Val loss: 0.884 | Gen: ethay arrwaway onditintingway isway orkingay\n",
            "Epoch:  59 | Train loss: 0.154 | Val loss: 0.890 | Gen: ethay arrwaway onditingintiay isway orkingway\n",
            "Epoch:  60 | Train loss: 0.183 | Val loss: 1.085 | Gen: ethay irrwaway onctiitintingway isway orkingway\n",
            "Epoch:  61 | Train loss: 0.423 | Val loss: 1.419 | Gen: eththay arraway ondcoiitingnay mastsay orkingay\n",
            "Epoch:  62 | Train loss: 0.464 | Val loss: 0.994 | Gen: ehay arrway ondetingitingway isay orkingay\n",
            "Epoch:  63 | Train loss: 0.311 | Val loss: 0.909 | Gen: ethay arrway onctioningingway isway orkingway\n",
            "Epoch:  64 | Train loss: 0.235 | Val loss: 0.785 | Gen: ethay arrway ondcoitingingway isway orkingway\n",
            "Epoch:  65 | Train loss: 0.182 | Val loss: 0.809 | Gen: ethay arrway onditingcay isway orkingay\n",
            "Epoch:  66 | Train loss: 0.164 | Val loss: 0.825 | Gen: ethay arrway onditingingway isway orkingay\n",
            "Epoch:  67 | Train loss: 0.151 | Val loss: 0.847 | Gen: ethay arrway onditingingway isway orkingway\n",
            "Epoch:  68 | Train loss: 0.143 | Val loss: 0.870 | Gen: ethay arrway onditingingway isway orkingway\n",
            "Epoch:  69 | Train loss: 0.135 | Val loss: 0.889 | Gen: ethay arrway onditingingway isway orkingway\n",
            "Epoch:  70 | Train loss: 0.130 | Val loss: 0.895 | Gen: ethay arrway onditingingway isway orkingway\n",
            "Epoch:  71 | Train loss: 0.124 | Val loss: 0.904 | Gen: ethay arrway onditingingway isway orkingway\n",
            "Epoch:  72 | Train loss: 0.119 | Val loss: 0.909 | Gen: ethay arrway onditingcay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.115 | Val loss: 0.924 | Gen: ethay arrway onditincay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.111 | Val loss: 0.926 | Gen: ethay arrway onditingcay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.107 | Val loss: 0.939 | Gen: ethay arrway onditincay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.103 | Val loss: 0.946 | Gen: ethay arrway onditincay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.099 | Val loss: 0.955 | Gen: ethay arrway onditincay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.096 | Val loss: 0.961 | Gen: ethay arrway onditincay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.093 | Val loss: 0.967 | Gen: ethay arrway onditincay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.089 | Val loss: 0.977 | Gen: ethay arrway onditincay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.086 | Val loss: 0.984 | Gen: ethay arrway onditincay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.083 | Val loss: 0.989 | Gen: ethay arrway onditincay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.080 | Val loss: 0.992 | Gen: ethay arrway onditincay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.077 | Val loss: 0.999 | Gen: ethay arrway onditincay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.075 | Val loss: 0.996 | Gen: ethay arrway onditincay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.072 | Val loss: 1.005 | Gen: ethay arrway onditincay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.071 | Val loss: 1.021 | Gen: ethay arrway onditincay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.112 | Val loss: 1.070 | Gen: ethay arrway onditioncingway isway orkingway\n",
            "Epoch:  89 | Train loss: 0.355 | Val loss: 1.178 | Gen: ehay arrway onditiingcay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.270 | Val loss: 0.878 | Gen: ehay arway onditingingway isway orkingway\n",
            "Epoch:  91 | Train loss: 0.155 | Val loss: 0.800 | Gen: ethay arrway onditingcay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.109 | Val loss: 0.845 | Gen: ethay arrway onditingntiay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.092 | Val loss: 0.876 | Gen: ethay arrway onditincay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.084 | Val loss: 0.910 | Gen: ethay arrway onditincay isway orkingway\n",
            "Epoch:  95 | Train loss: 0.080 | Val loss: 0.891 | Gen: ethay arrway onditincay isway orkingway\n",
            "Epoch:  96 | Train loss: 0.074 | Val loss: 0.883 | Gen: ethay arrway onditincay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.071 | Val loss: 0.887 | Gen: ethay arrway onditincay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.067 | Val loss: 0.890 | Gen: ethay arrway onditincay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.065 | Val loss: 0.896 | Gen: ethay arrway onditincay isway orkingway\n",
            "Obtained lowest validation loss of: 0.7849285908043384\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay arrway onditincay isway orkingway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "trans32_args_s = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 100,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 100,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 32,\n",
        "    \"encoder_type\": \"transformer\",\n",
        "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
        "    \"num_transformer_layers\": 4,\n",
        "}\n",
        "trans32_args_s.update(args_dict)\n",
        "print_opts(trans32_args_s)\n",
        "\n",
        "trans32_encoder_s, trans32_decoder_s, trans32_losses_s = train(trans32_args_s)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l28mKuZxvaRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac421682-e97d-46fe-ac02-6045003301da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay arrway onditincay isway orkingway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L8EqLYFu48H"
      },
      "source": [
        "In the following cells, we investigate the effects of increasing model size and dataset size on the training / validation curves and generalization of the Transformer. We will increase hidden size to 64, and also increase dataset size. Include the best achieved validation loss in your report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdZO69DozuUu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a44d87ad-8a45-482d-ec89-28716e9e89b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 10                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('stirling', 'irlingstay')\n",
            "('fundamentally', 'undamentallyfay')\n",
            "('respectful', 'espectfulray')\n",
            "('osborne', 'osborneway')\n",
            "('companions', 'ompanionscay')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.661 | Val loss: 2.253 | Gen: ay ay oiway-iway iway oiway-eray\n",
            "Epoch:   1 | Train loss: 2.065 | Val loss: 2.002 | Gen: eay ay onway-inway iway onway-ay\n",
            "Epoch:   2 | Train loss: 1.835 | Val loss: 1.891 | Gen: uay aaay onway iway onway\n",
            "Epoch:   3 | Train loss: 1.667 | Val loss: 1.769 | Gen: eay away-ay-ay-ay incay-ionay isay ogway-inay\n",
            "Epoch:   4 | Train loss: 1.547 | Val loss: 1.673 | Gen: eay away-iway incincay iay ogway-ickay\n",
            "Epoch:   5 | Train loss: 1.461 | Val loss: 1.621 | Gen: etay-eway arway-iray ongingngtioncay isay oggingray\n",
            "Epoch:   6 | Train loss: 1.387 | Val loss: 1.532 | Gen: eway away-iray ongtingway isay otingway\n",
            "Epoch:   7 | Train loss: 1.302 | Val loss: 1.493 | Gen: eway aray ongingway iay orkingray\n",
            "Epoch:   8 | Train loss: 1.249 | Val loss: 1.523 | Gen: etay iray ongtingy-iongy isw orgrgr-ing\n",
            "Epoch:   9 | Train loss: 1.219 | Val loss: 1.404 | Gen: eway arway ongingway isway orgrgray\n",
            "Epoch:  10 | Train loss: 1.144 | Val loss: 1.362 | Gen: eway aray ondingay-iorday isay orkingray\n",
            "Epoch:  11 | Train loss: 1.079 | Val loss: 1.306 | Gen: eway arway ondgtingway isay orkingway\n",
            "Epoch:  12 | Train loss: 1.040 | Val loss: 1.283 | Gen: eway aray ondgtingway isay orkingway\n",
            "Epoch:  13 | Train loss: 1.004 | Val loss: 1.258 | Gen: eway arway ondgtinay isway orkingway\n",
            "Epoch:  14 | Train loss: 0.972 | Val loss: 1.234 | Gen: eway aray ondgtinay isay orkingway\n",
            "Epoch:  15 | Train loss: 0.944 | Val loss: 1.232 | Gen: eway arway ondgtinay isway orkingway\n",
            "Epoch:  16 | Train loss: 0.925 | Val loss: 1.164 | Gen: eway aray onditingay isay orkingway\n",
            "Epoch:  17 | Train loss: 0.891 | Val loss: 1.179 | Gen: eway aray ondintinay isway orkingnay\n",
            "Epoch:  18 | Train loss: 0.892 | Val loss: 1.193 | Gen: thay iaray onditinatinay isay orkingay\n",
            "Epoch:  19 | Train loss: 0.848 | Val loss: 1.127 | Gen: eway aray ondinatinay isay orkingway\n",
            "Epoch:  20 | Train loss: 0.801 | Val loss: 1.076 | Gen: eway iaray ondinationgay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.769 | Val loss: 1.069 | Gen: eway iaray onditingway isay orkingway\n",
            "Epoch:  22 | Train loss: 0.746 | Val loss: 1.090 | Gen: ewhay iaray ondinatingay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.725 | Val loss: 1.021 | Gen: eway iaray ondinatingway isway orkingway\n",
            "Epoch:  24 | Train loss: 0.690 | Val loss: 1.014 | Gen: ewhay arway ondinationgay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.669 | Val loss: 0.983 | Gen: ewhay iaray onditingway isway orkingway\n",
            "Epoch:  26 | Train loss: 0.645 | Val loss: 0.973 | Gen: ewhay iraray ondinationgay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.629 | Val loss: 0.940 | Gen: ewhay iaray onditingway isway orkingway\n",
            "Epoch:  28 | Train loss: 0.617 | Val loss: 0.953 | Gen: ehthay ariraway onditingay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.602 | Val loss: 0.972 | Gen: ehtay ariray onditingay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.598 | Val loss: 1.049 | Gen: ehthay iraray onditingnay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.605 | Val loss: 0.997 | Gen: ehtay iaray onditingay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.595 | Val loss: 0.996 | Gen: ehtay irway onditingcontingway isway orkingray\n",
            "Epoch:  33 | Train loss: 0.548 | Val loss: 0.872 | Gen: ehthay ariway onditingcontingway isway orkingway\n",
            "Epoch:  34 | Train loss: 0.513 | Val loss: 0.816 | Gen: ehthay ariway onditingay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.488 | Val loss: 0.802 | Gen: ehthay ariway onditingcontingway isway orkingway\n",
            "Epoch:  36 | Train loss: 0.472 | Val loss: 0.790 | Gen: ehtay ariway onditingcontay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.457 | Val loss: 0.785 | Gen: ehtay ariway onditingcontay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.444 | Val loss: 0.775 | Gen: ehtay ariway onditingconinay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.433 | Val loss: 0.758 | Gen: ehtay ariway onditingconinway isway orkingway\n",
            "Epoch:  40 | Train loss: 0.420 | Val loss: 0.770 | Gen: ehtay ariway onditingconincay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.412 | Val loss: 0.754 | Gen: ehtay ariway onditingconinway isway orkingway\n",
            "Epoch:  42 | Train loss: 0.403 | Val loss: 0.785 | Gen: ehtay ariway onditingconincay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.409 | Val loss: 0.816 | Gen: ehtay ariwaway onditingcontay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.408 | Val loss: 0.818 | Gen: ehtay ariway onditingconincay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.383 | Val loss: 0.747 | Gen: ehtay ariway onditinoningway isway orkingway\n",
            "Epoch:  46 | Train loss: 0.365 | Val loss: 0.724 | Gen: ehtay ariway onditingconincay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.349 | Val loss: 0.745 | Gen: ehtay ariway onditingconincay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.338 | Val loss: 0.725 | Gen: ehtay ariway onditingconincay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.327 | Val loss: 0.705 | Gen: ehtay ariway onditingconincay isway orkingway\n",
            "Epoch:  50 | Train loss: 0.318 | Val loss: 0.678 | Gen: ehtay ariway onditingconincay isway orkingway\n",
            "Epoch:  51 | Train loss: 0.323 | Val loss: 0.840 | Gen: ehtay irwaway onditingcontay isway orkingray\n",
            "Epoch:  52 | Train loss: 0.346 | Val loss: 0.705 | Gen: ehtay ariway onditingconicay isway orkingway\n",
            "Epoch:  53 | Train loss: 0.331 | Val loss: 0.705 | Gen: ehtay ariway onditiongcinway isway orkingway\n",
            "Epoch:  54 | Train loss: 0.335 | Val loss: 0.723 | Gen: ehtay ariway onditiongcay isway orkingway\n",
            "Epoch:  55 | Train loss: 0.347 | Val loss: 0.774 | Gen: ehtay ariray onditinioningcay isway orkingway\n",
            "Epoch:  56 | Train loss: 0.324 | Val loss: 0.651 | Gen: ehtay ariwaway onditingconay isway orkingray\n",
            "Epoch:  57 | Train loss: 0.292 | Val loss: 0.619 | Gen: ehthay ariway onditingconay isway orkingway\n",
            "Epoch:  58 | Train loss: 0.268 | Val loss: 0.604 | Gen: ehtay ariway onditingconinway isway orkingway\n",
            "Epoch:  59 | Train loss: 0.257 | Val loss: 0.610 | Gen: ehtay ariway onditinoningcay isway orkingway\n",
            "Epoch:  60 | Train loss: 0.248 | Val loss: 0.597 | Gen: ehtay ariway onditingconincay isway orkingway\n",
            "Epoch:  61 | Train loss: 0.241 | Val loss: 0.604 | Gen: ehtay ariway onditinoningcay isway orkingway\n",
            "Epoch:  62 | Train loss: 0.235 | Val loss: 0.585 | Gen: ehtay ariway onditingconincay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.230 | Val loss: 0.607 | Gen: ehtay ariway onditinoningcay isway orkingway\n",
            "Epoch:  64 | Train loss: 0.225 | Val loss: 0.583 | Gen: ehtay ariway onditingconincay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.218 | Val loss: 0.595 | Gen: ehtay ariway onditinoningcay isway orkingway\n",
            "Epoch:  66 | Train loss: 0.212 | Val loss: 0.575 | Gen: ehtay ariway onditingconincay isway orkingway\n",
            "Epoch:  67 | Train loss: 0.207 | Val loss: 0.576 | Gen: ehtay ariway onditinoningcay isway orkingway\n",
            "Epoch:  68 | Train loss: 0.201 | Val loss: 0.568 | Gen: ehtay ariway onditingconincay isway orkingway\n",
            "Epoch:  69 | Train loss: 0.196 | Val loss: 0.571 | Gen: ehtay ariway onditinoningcay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.191 | Val loss: 0.557 | Gen: ehtay ariway onditinoningcay isway orkingway\n",
            "Epoch:  71 | Train loss: 0.187 | Val loss: 0.572 | Gen: ehtay ariway onditinoningcay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.183 | Val loss: 0.546 | Gen: ehtay ariway onditingcotway isway orkingway\n",
            "Epoch:  73 | Train loss: 0.181 | Val loss: 0.575 | Gen: ehtay airway onditinoningcay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.175 | Val loss: 0.551 | Gen: ehtay ariway onditingcotway isway orkingway\n",
            "Epoch:  75 | Train loss: 0.170 | Val loss: 0.558 | Gen: ehtay arirway onditinoningcay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.166 | Val loss: 0.540 | Gen: ehtay ariway onditinotingcay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.160 | Val loss: 0.534 | Gen: ehtay airway onditinotingcay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.156 | Val loss: 0.534 | Gen: ehtay airway onditinoningcay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.151 | Val loss: 0.532 | Gen: ehtay airway onditinotingcay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.147 | Val loss: 0.521 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.144 | Val loss: 0.540 | Gen: ethay airway onditinoningcay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.139 | Val loss: 0.537 | Gen: ethay airway onditiongcincay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.135 | Val loss: 0.540 | Gen: ethay airway onditiongcincay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.134 | Val loss: 0.547 | Gen: ethay airway onditinoningcay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.146 | Val loss: 0.536 | Gen: ethay airway ondititiongcay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.434 | Val loss: 0.952 | Gen: thay-ataay airway onditinationgcay isay orkingway\n",
            "Epoch:  87 | Train loss: 0.372 | Val loss: 0.540 | Gen: ehtay irrway onditioningcay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.222 | Val loss: 0.521 | Gen: ehtay ariway onditioningcay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.178 | Val loss: 0.467 | Gen: ehtay ariway onditioningcay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.158 | Val loss: 0.449 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.149 | Val loss: 0.452 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.140 | Val loss: 0.444 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.142 | Val loss: 0.443 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.147 | Val loss: 0.450 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  95 | Train loss: 0.138 | Val loss: 0.455 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  96 | Train loss: 0.130 | Val loss: 0.440 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.124 | Val loss: 0.443 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.118 | Val loss: 0.436 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.114 | Val loss: 0.437 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Obtained lowest validation loss of: 0.43609637812098334\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehtay airway onditioningcay isway orkingway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "trans32_args_l = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_large\",  # Increased data set size\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 100,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 10,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 512,\n",
        "    \"hidden_size\": 32,\n",
        "    \"encoder_type\": \"transformer\",\n",
        "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
        "    \"num_transformer_layers\": 3,\n",
        "}\n",
        "trans32_args_l.update(args_dict)\n",
        "print_opts(trans32_args_l)\n",
        "\n",
        "trans32_encoder_l, trans32_decoder_l, trans32_losses_l = train(trans32_args_l)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans32_encoder_l, trans32_decoder_l, None, trans32_args_l\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmoTgrDcr_dw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e3f2fe-0b74-4e1e-9a4a-530cf1670688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 20                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('gales', 'alesgay')\n",
            "('palanquins', 'alanquinspay')\n",
            "('measuring', 'easuringmay')\n",
            "('weather', 'eatherway')\n",
            "('closing', 'osingclay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.356 | Val loss: 1.943 | Gen: eway iray incccccccccccccccccc isway ingngray-ay\n",
            "Epoch:   1 | Train loss: 1.674 | Val loss: 1.957 | Gen: eway iway ingay-ay iway ondway\n",
            "Epoch:   2 | Train loss: 1.473 | Val loss: 1.531 | Gen: ethay irrway ongay-ingay isway odgngway\n",
            "Epoch:   3 | Train loss: 1.223 | Val loss: 1.469 | Gen: eatay-ay araway oongay-iongay isay odgay-ay\n",
            "Epoch:   4 | Train loss: 1.056 | Val loss: 1.301 | Gen: etay arway oingay-iongay isway orkkngway\n",
            "Epoch:   5 | Train loss: 0.915 | Val loss: 1.110 | Gen: ehay irway oindiongay isway odway-inway\n",
            "Epoch:   6 | Train loss: 0.793 | Val loss: 1.005 | Gen: ethay arway ondingay-ionday isway orkingway\n",
            "Epoch:   7 | Train loss: 0.678 | Val loss: 1.188 | Gen: etay aiway ondingay-inday isway okngway-ay\n",
            "Epoch:   8 | Train loss: 0.682 | Val loss: 0.950 | Gen: ethay arway ondidiongay isway orway-ingnway\n",
            "Epoch:   9 | Train loss: 0.567 | Val loss: 0.934 | Gen: ethay arway oodingctiongday isway owkringway\n",
            "Epoch:  10 | Train loss: 0.517 | Val loss: 0.900 | Gen: etay arway onditiongingay isway orway-ingway\n",
            "Epoch:  11 | Train loss: 0.458 | Val loss: 0.852 | Gen: ethay arway onditingcay isway owrway-ingway\n",
            "Epoch:  12 | Train loss: 0.381 | Val loss: 0.806 | Gen: ethay arirway ondingiongcay isway owkringway\n",
            "Epoch:  13 | Train loss: 0.350 | Val loss: 0.816 | Gen: ethay arway ondingingcayingay isway orkrwingway\n",
            "Epoch:  14 | Train loss: 0.324 | Val loss: 0.793 | Gen: ethay-ay arway onditionginay isway okingway\n",
            "Epoch:  15 | Train loss: 0.323 | Val loss: 0.938 | Gen: etay arway ondinitingcay-iongio isay orkingway-ingway\n",
            "Epoch:  16 | Train loss: 0.283 | Val loss: 0.778 | Gen: ethay arrway onditiongngcaycay isway orkingway\n",
            "Epoch:  17 | Train loss: 0.234 | Val loss: 0.722 | Gen: ethay arrway onditiongngcay isway orkwingwngway\n",
            "Epoch:  18 | Train loss: 0.210 | Val loss: 0.881 | Gen: ethay arrway ondinngcongngnay isway orkingngway\n",
            "Epoch:  19 | Train loss: 0.195 | Val loss: 0.747 | Gen: ethay arrway ondintiongnongcay isway orkringway\n",
            "Epoch:  20 | Train loss: 0.166 | Val loss: 0.604 | Gen: ethay arirway onditiongcingcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.156 | Val loss: 0.684 | Gen: ettay irrway onditiongingcay isway orkringway\n",
            "Epoch:  22 | Train loss: 0.144 | Val loss: 0.624 | Gen: ehtay arrway onditiongingcay isay orkingway\n",
            "Epoch:  23 | Train loss: 0.119 | Val loss: 0.619 | Gen: ethay arirway oninditingcay isway orkingwray\n",
            "Epoch:  24 | Train loss: 0.124 | Val loss: 0.633 | Gen: ethay arrway onditiongingcay isway orkrwngway\n",
            "Epoch:  25 | Train loss: 0.114 | Val loss: 0.676 | Gen: ethay arrway onditiongingcay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.118 | Val loss: 0.669 | Gen: ethay arrway onditioningcay isway orkringway\n",
            "Epoch:  27 | Train loss: 0.115 | Val loss: 0.608 | Gen: etay irrway onditioningioway isway orkingway\n",
            "Epoch:  28 | Train loss: 0.095 | Val loss: 0.671 | Gen: ethay arrway onditiongcay isway orkingwray\n",
            "Epoch:  29 | Train loss: 0.094 | Val loss: 0.722 | Gen: ethay arrway ondiditingingcay isway orkingwray\n",
            "Epoch:  30 | Train loss: 0.152 | Val loss: 0.757 | Gen: ethay arrway onditioningcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.140 | Val loss: 0.846 | Gen: ehtay arrway onditiongingngcay isay orkngray\n",
            "Epoch:  32 | Train loss: 0.237 | Val loss: 1.638 | Gen: eay arway oninitingingay isay orkingay\n",
            "Epoch:  33 | Train loss: 0.487 | Val loss: 0.853 | Gen: ethay arirway oncidtiongcay iswiy orkingway\n",
            "Epoch:  34 | Train loss: 0.244 | Val loss: 0.578 | Gen: ethay arirway onditiongcatingcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.133 | Val loss: 0.469 | Gen: ethay arirway onditiongingcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.076 | Val loss: 0.453 | Gen: eethay arirway onditiongingcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.058 | Val loss: 0.439 | Gen: ethay arirway onditiongingcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.048 | Val loss: 0.452 | Gen: ethay arirway onditiongingcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.039 | Val loss: 0.445 | Gen: ethay arirway onditiongingcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.032 | Val loss: 0.445 | Gen: ethay arirway onditiongingcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.028 | Val loss: 0.449 | Gen: ethay arirway onditiongingcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.024 | Val loss: 0.457 | Gen: ethay arirway onditiongingcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.022 | Val loss: 0.459 | Gen: ethay arirway onditiongingcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.019 | Val loss: 0.461 | Gen: ethay arirway onditiongingcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.020 | Val loss: 0.480 | Gen: ethay arirway onditiongingcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.032 | Val loss: 0.765 | Gen: ehttay arirway onditioningcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.103 | Val loss: 0.633 | Gen: ehtay arrway onditiongingcay isway orkwingway\n",
            "Epoch:  48 | Train loss: 0.089 | Val loss: 0.562 | Gen: ethay arway onditioningcay isway okwringway\n",
            "Epoch:  49 | Train loss: 0.096 | Val loss: 0.602 | Gen: ethay arway onditiongcay isway orkingway\n",
            "Obtained lowest validation loss of: 0.43915095590054987\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay arway onditiongcay isway orkingway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "trans64_args_s = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 50,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 20,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 64,  # Increased model size\n",
        "    \"encoder_type\": \"transformer\",\n",
        "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
        "    \"num_transformer_layers\": 3,\n",
        "}\n",
        "trans64_args_s.update(args_dict)\n",
        "print_opts(trans64_args_s)\n",
        "\n",
        "trans64_encoder_s, trans64_decoder_s, trans64_losses_s = train(trans64_args_s)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans64_encoder_s, trans64_decoder_s, None, trans64_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dardK4RWvUWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78462987-c2c5-4b82-b77d-9d5e4f622dce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 20                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('stirling', 'irlingstay')\n",
            "('fundamentally', 'undamentallyfay')\n",
            "('respectful', 'espectfulray')\n",
            "('osborne', 'osborneway')\n",
            "('companions', 'ompanionscay')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.394 | Val loss: 1.997 | Gen: tay inway oootinonooononay issay otingway\n",
            "Epoch:   1 | Train loss: 1.706 | Val loss: 1.800 | Gen: away iway oootininationay isay ooggway\n",
            "Epoch:   2 | Train loss: 1.478 | Val loss: 1.672 | Gen: etay-ay-ay away ootay-ingay isay oggggway\n",
            "Epoch:   3 | Train loss: 1.324 | Val loss: 1.590 | Gen: ththay irray oincincionay isaysaysisay oringway\n",
            "Epoch:   4 | Train loss: 1.165 | Val loss: 1.375 | Gen: eway iway ontionionay isway oway\n",
            "Epoch:   5 | Train loss: 1.086 | Val loss: 1.390 | Gen: eway iway ondingay isway oway\n",
            "Epoch:   6 | Train loss: 0.995 | Val loss: 1.557 | Gen: eway iway onicay isway owaygay\n",
            "Epoch:   7 | Train loss: 0.934 | Val loss: 1.160 | Gen: eway iray ondicinicioncay isway oringway\n",
            "Epoch:   8 | Train loss: 0.798 | Val loss: 1.229 | Gen: eaway iraway ondicionicay isay orkay\n",
            "Epoch:   9 | Train loss: 0.755 | Val loss: 0.967 | Gen: eay array ondicingcay isay orkay\n",
            "Epoch:  10 | Train loss: 0.664 | Val loss: 0.900 | Gen: eway iray ondicicingcay isway orkingway\n",
            "Epoch:  11 | Train loss: 0.556 | Val loss: 0.869 | Gen: eway ariway ondicingcay isay owkray\n",
            "Epoch:  12 | Train loss: 0.552 | Val loss: 1.054 | Gen: eeway ariway ondintingway isway orrrkingway\n",
            "Epoch:  13 | Train loss: 0.562 | Val loss: 0.952 | Gen: ehay ariway ondiciticicangckgcay isway orkinggway\n",
            "Epoch:  14 | Train loss: 0.473 | Val loss: 0.722 | Gen: ethay airway ondicitiongway isway okingway\n",
            "Epoch:  15 | Train loss: 0.390 | Val loss: 0.644 | Gen: ehay airway ondictiongingway isay orkingway\n",
            "Epoch:  16 | Train loss: 0.346 | Val loss: 0.690 | Gen: ethay ariway onditicingingway isway orkingway\n",
            "Epoch:  17 | Train loss: 0.341 | Val loss: 0.641 | Gen: ethay airway onditicingcay isway okingkway\n",
            "Epoch:  18 | Train loss: 0.308 | Val loss: 0.958 | Gen: ethay airway odiziciongiticay isay orkikgway\n",
            "Epoch:  19 | Train loss: 0.333 | Val loss: 0.603 | Gen: ethay airway onditicinginay isay orkingway\n",
            "Epoch:  20 | Train loss: 0.269 | Val loss: 0.544 | Gen: ethay airway onditiongicay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.250 | Val loss: 0.670 | Gen: eway iaway ondictiongingcay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.260 | Val loss: 0.652 | Gen: ethay ariwwwwwwwwwayway onditiongingway iswayy orkingway\n",
            "Epoch:  23 | Train loss: 0.229 | Val loss: 0.515 | Gen: ethay airway ondictiongingcay isway okingway\n",
            "Epoch:  24 | Train loss: 0.192 | Val loss: 0.496 | Gen: ethay ariway onditionicay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.165 | Val loss: 0.440 | Gen: ethay airway onditicingncay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.147 | Val loss: 0.391 | Gen: ethay airway onditiongicay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.121 | Val loss: 0.365 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.108 | Val loss: 0.363 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.094 | Val loss: 0.357 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.089 | Val loss: 0.400 | Gen: ethay airway onditiontingcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.162 | Val loss: 1.533 | Gen: thay irway onditttintingway isway orkingway\n",
            "Epoch:  32 | Train loss: 0.465 | Val loss: 1.210 | Gen: hetay awaywaywaywaywaywayw ondicay iswayway orkingwwwwwwwwwwwwww\n",
            "Epoch:  33 | Train loss: 0.373 | Val loss: 0.573 | Gen: ethay airway ondictiongcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.208 | Val loss: 0.504 | Gen: ehthay arway onditiongincay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.149 | Val loss: 0.341 | Gen: ethay airway onditionicay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.104 | Val loss: 0.320 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.090 | Val loss: 0.340 | Gen: ethay ariway onditioningcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.076 | Val loss: 0.291 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.063 | Val loss: 0.278 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.053 | Val loss: 0.275 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.047 | Val loss: 0.269 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.043 | Val loss: 0.269 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.039 | Val loss: 0.264 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.037 | Val loss: 0.266 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.035 | Val loss: 0.259 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.034 | Val loss: 0.265 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.031 | Val loss: 0.256 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.030 | Val loss: 0.262 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.027 | Val loss: 0.255 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Obtained lowest validation loss of: 0.25479348254276024\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "trans64_args_l = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_large\",  # Increased data set size\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 50,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 20,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 512,\n",
        "    \"hidden_size\": 64,  # Increased model size\n",
        "    \"encoder_type\": \"transformer\",\n",
        "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
        "    \"num_transformer_layers\": 3,\n",
        "}\n",
        "trans64_args_l.update(args_dict)\n",
        "print_opts(trans64_args_l)\n",
        "\n",
        "trans64_encoder_l, trans64_decoder_l, trans64_losses_l = train(trans64_args_l)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans64_encoder_l, trans64_decoder_l, None, trans64_args_l\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSSyiG39vVlN"
      },
      "source": [
        "The following cell generates two loss plots. In the first plot, we compare the effects of increasing dataset size. In the second plot, we compare the effects of increasing model size. Include both plots in your report, and include your analysis of the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ql0pxrEvVP6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f7c3a1b-4361-421d-a28d-916f54e7fb7a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "save_loss_comparison_by_dataset(\n",
        "    trans32_losses_s,\n",
        "    trans32_losses_l,\n",
        "    trans64_losses_s,\n",
        "    trans64_losses_l,\n",
        "    trans32_args_s,\n",
        "    trans32_args_l,\n",
        "    trans64_args_s,\n",
        "    trans64_args_l,\n",
        "    \"trans_by_dataset\",\n",
        ")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_loss_comparison_by_hidden(\n",
        "    trans32_losses_s,\n",
        "    trans32_losses_l,\n",
        "    trans64_losses_s,\n",
        "    trans64_losses_l,\n",
        "    trans32_args_s,\n",
        "    trans32_args_l,\n",
        "    trans64_args_s,\n",
        "    trans64_args_l,\n",
        "    \"trans_by_hidden\",\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ZJ_2SF1tCaUy",
        "outputId": "cf60dd72-7e69-4324-ec4a-f3cbde7a635d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Written Response\n",
        "5.\n",
        " **Comparisons with RNNAttention**\n",
        "```\n",
        "Lowest validation loss from transformers: 0.7849285908043384\n",
        "Lowest validation loss from RNNAttention: 0.20935316750994667\n",
        "```\n",
        "As you can see, from the above, the transformer model perform worse than the RNNAttention model, since the lowest validation loss obtained from transformers is higher than that of the RNNAttention model. In terms of training time however, the transformer model's training time is less than that of the RNN Attention model.\n",
        "\n",
        "**Comparisons with Single Block Attention**\n",
        "```\n",
        "Lowest validation loss from transformers: 0.7849285908043384\n",
        "Lowest validation loss from Single Block Attention: 1.1933680046827366\n",
        "```\n",
        "From the above, we can see that the transformers have a lower lowest validation loss than that of the single block attention. In terms of training speed, the transformer model is slower than the single block attention model.\n",
        "\n"
      ],
      "metadata": {
        "id": "Gf_BgCAZG76G"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "TjPTaRB4mpCd",
        "s9IS9B9-yUU5",
        "9DaTdRNuUra7",
        "pbvpn4MaV0I1",
        "bRWfRdmVVjUl",
        "0yh08KhgnA30",
        "ecEq4TP2lZ4Z"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}